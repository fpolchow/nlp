{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are a few different NLP libraries with python, uncomment the code below to install them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install spacy\n",
    "# ! python -m spacy download en\n",
    "# import nltk\n",
    "# nltk.download('punkt') ##this downloads the default word tokenizer\n",
    "# nltk.download('stopwords') ##this downloads all stopwords\n",
    "# nltk.download('popular') ##this downloads many different popular libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization (stop words, stemming, lemmatizing)\n",
    "### 2. Vectorization\n",
    "### 3. Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key terminology:\n",
    "\n",
    "* Document: each individual body of text you are observing\n",
    "* Corpus: the entire collection of texts\n",
    "* Tokenization: the process by which you break up each document into smaller pieces. You can tokenize into sentences or words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at a collection of Reddit comments from early 2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('./RC_2006-01.bz2',compression='bz2',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>edited</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>score</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>ups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jh99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>early 2006 a probable date</td>\n",
       "      <td>0</td>\n",
       "      <td>1136074029</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>c2715</td>\n",
       "      <td>t3_22569</td>\n",
       "      <td>t3_22569</td>\n",
       "      <td>1473821517</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>t5_6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jpb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>If you are going to post something that has a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1136076410</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>c2717</td>\n",
       "      <td>t3_22542</td>\n",
       "      <td>t3_22542</td>\n",
       "      <td>1473821517</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>t5_6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pichu0102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Microsoft hates it's own products?\\r\\nWho knew?</td>\n",
       "      <td>0</td>\n",
       "      <td>1136078623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>c2718</td>\n",
       "      <td>t3_22515</td>\n",
       "      <td>t3_22515</td>\n",
       "      <td>1473821517</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>t5_6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>libertas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this looks interesting, but it's already aired...</td>\n",
       "      <td>0</td>\n",
       "      <td>1136079346</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>c2719</td>\n",
       "      <td>t3_22528</td>\n",
       "      <td>t3_22528</td>\n",
       "      <td>1473821517</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>t5_6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mdmurray</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I have nothing but good things to say about De...</td>\n",
       "      <td>0</td>\n",
       "      <td>1136081389</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>c2722</td>\n",
       "      <td>t3_22538</td>\n",
       "      <td>t3_22538</td>\n",
       "      <td>1473821517</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>t5_6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      author  author_flair_css_class  author_flair_text  \\\n",
       "0       jh99                     NaN                NaN   \n",
       "1        jpb                     NaN                NaN   \n",
       "2  Pichu0102                     NaN                NaN   \n",
       "3   libertas                     NaN                NaN   \n",
       "4   mdmurray                     NaN                NaN   \n",
       "\n",
       "                                                body  controversiality  \\\n",
       "0                         early 2006 a probable date                 0   \n",
       "1  If you are going to post something that has a ...                 0   \n",
       "2    Microsoft hates it's own products?\\r\\nWho knew?                 0   \n",
       "3  this looks interesting, but it's already aired...                 0   \n",
       "4  I have nothing but good things to say about De...                 0   \n",
       "\n",
       "   created_utc  distinguished  edited  gilded     id   link_id parent_id  \\\n",
       "0   1136074029            NaN   False       0  c2715  t3_22569  t3_22569   \n",
       "1   1136076410            NaN   False       0  c2717  t3_22542  t3_22542   \n",
       "2   1136078623            NaN   False       0  c2718  t3_22515  t3_22515   \n",
       "3   1136079346            NaN   False       0  c2719  t3_22528  t3_22528   \n",
       "4   1136081389            NaN   False       0  c2722  t3_22538  t3_22538   \n",
       "\n",
       "   retrieved_on  score  stickied   subreddit subreddit_id  ups  \n",
       "0    1473821517      0     False  reddit.com         t5_6    0  \n",
       "1    1473821517      0     False  reddit.com         t5_6    0  \n",
       "2    1473821517      2     False  reddit.com         t5_6    2  \n",
       "3    1473821517      2     False  reddit.com         t5_6    2  \n",
       "4    1473821517      0     False  reddit.com         t5_6    0  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "documents = df['body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization\n",
    "Let's tokenize each of the documents. Let's try the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If',\n",
       " 'you',\n",
       " 'are',\n",
       " 'going',\n",
       " 'to',\n",
       " 'post',\n",
       " 'something',\n",
       " 'that',\n",
       " 'has',\n",
       " 'a',\n",
       " 'link',\n",
       " 'to',\n",
       " 'the',\n",
       " 'original',\n",
       " 'author',\n",
       " ',',\n",
       " 'why',\n",
       " 'not',\n",
       " 'just',\n",
       " 'post',\n",
       " 'the',\n",
       " 'original',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'someone',\n",
       " \"'s\",\n",
       " 'copy',\n",
       " '?']"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "word_tokenize(documents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have capitalization, punctuation and words that are all too frequent, such as \"a\", \"the\", \"two\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "my_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def tokenize(document):\n",
    "    tocs = word_tokenize(document.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokenized_list = [toc for toc in tocs if toc not in stop_words]\n",
    "    \n",
    "    \n",
    "    \n",
    "    punctuation = set(string.punctuation)\n",
    "    no_punctuation = [word for word in tokenized_list if word not in punctuation]\n",
    "    \n",
    "   \n",
    "    \n",
    "    return no_punctuation\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['because',\n",
       " 'this',\n",
       " 'one',\n",
       " 'is',\n",
       " 'different',\n",
       " 'and',\n",
       " 'i',\n",
       " 'like',\n",
       " 'it',\n",
       " 'better',\n",
       " 'here',\n",
       " \"'s\",\n",
       " 'your',\n",
       " 'sign']"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(documents[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OMG',\n",
       " 'this',\n",
       " 'is',\n",
       " '#',\n",
       " 'bigly',\n",
       " 'fun',\n",
       " 'and',\n",
       " '#',\n",
       " 'bigly',\n",
       " 'cool',\n",
       " ',',\n",
       " 'lets',\n",
       " 'go',\n",
       " 'running',\n",
       " '!']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = nltk.word_tokenize('OMG this is #bigly fun and #bigly cool, lets go running!')\n",
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer\n",
    "twt = TweetTokenizer()\n",
    "one_tweet = twt.tokenize('OMG this is so #fun and #bigly cool. SAD!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OMG', 'this', 'is', 'so', '#fun', 'and', '#bigly', 'cool', '.', 'SAD', '!']"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also tokenize by sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is my first sentence.',\n",
       " 'This is my second sentence.',\n",
       " 'Oh wow now there is a third sentence.',\n",
       " 'This is getting out of control!']"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "test_text = \"\"\"This is my first sentence. This is my second sentence. Oh wow now there is a third\\\n",
    " sentence. This is getting out of control!\"\"\"\n",
    "sent_tokenize(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming/ Lemmatization\n",
    "\n",
    "* Stemming: reduces words by removing suffixes (often reduces to strings that are not real words)\n",
    "* Lemmatization: reduces words to some root form that is still in the the English dictionary\n",
    "\n",
    "Longer Explanation: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "\n",
    "Your use of stemming/lemmatization will wholly depend on the context of the problem you're trying to solve. Let's take a look at how a sample sentence might be treated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_prodigy = documents[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(document,stemmer):\n",
    "    toks = nltk.word_tokenize(document)\n",
    "    wrd_list = []\n",
    "    for word in toks:\n",
    "        wrd_list.append(stemmer.stem(word))\n",
    "    return \" \".join(wrd_list)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the reason the music industri is loos money is becaus they focus to much on dispos music . they find the front men ( or women ) to sing , get some music made by peopl who know onli how to make music that is the sound of the `` moment '' . everyon know their role , band member just do what their told to . then this shit get heavili market and peopl buy it , but buy audienc is small and this stuff onli get brought while it is in fashion . compar that to music that get to where it is through talent and passion . when i hear a market band singer sing about , say loss , it just ca n't reach me becaus it 's not true and is sung without mean . compar that to a real band where it 's the singer real experi . music like this can continu to sell for decad and by all generat . if the music industri want there sale to go up they should go to the club and do talent spot like they use to and not by peopl who think what will sell but who think what sound good and more to the point what *feels* good .\""
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words(music_prodigy,snowball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the reason the mus industry is loos money is becaus they foc to much on dispos mus . they find the front men ( or wom ) to sing , get som mus mad by peopl who know on how to mak mus that is the sound of the `` mom '' . everyon know their rol , band memb just do what their told to . then thi shit get heavy market and peopl buy it , but buy audy is smal and thi stuff on get brought whil it is in fash . comp that to mus that get to wher it is through tal and pass . when i hear a market band sing sing about , say loss , it just ca n't reach me becaus it 's not tru and is sung without mean . comp that to a real band wher it 's the sing real expery . mus lik thi can continu to sel for decad and by al gen . if the mus industry want ther sal to go up they should go to the club and do tal spot lik they us to and not by peopl who think what wil sel but who think what sound good and mor to the point what *feels* good .\""
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words(music_prodigy,lancaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.RegexpStemmer('ing$|s$|e$|able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem_words(document,lemmer):\n",
    "    toks = nltk.word_tokenize(document)\n",
    "    wrd_list = []\n",
    "    for word in toks:\n",
    "        wrd_list.append(lemmer.lemmatize(word))\n",
    "    return \" \".join(wrd_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "lemmer.lemmatize('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The reason the music industry is loosing money is because they focus to much on disposable music . They find the front men ( or woman ) to sing , get some music made by people who know only how to make music that is the sound of the `` moment '' . Everyone know their role , band member just do what their told to . Then this shit get heavily marketed and people buy it , but buying audience is small and this stuff only get brought while it is in fashion . Compare that to music that get to where it is through talent and passion . When I hear a marketed band singer singing about , say loss , It just ca n't reach me because it 's not true and is sung without meaning . Compared that to a real band where it 's the singer real experience . Music like this can continue to sell for decade and by all generation . If the music industry want there sale to go up they should go to the club and do talent spotting like they used to and not by people who think what will sell but who think what sound good and more to the point what *feels* good .\""
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_words(music_prodigy,lemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "#### Machine Learning models aren't able to operate on text because text means nothing to mathematical functions! We need to convert our text to a numerical form. SciKit Learn has a package that quickly vectorizes your text.\n",
    "\n",
    "Let's look at CountVectorizer first, which creates a Bag of Words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We are going to fit vectorizer with specifications to a specific corpus\n",
    "## there are many different parameters you can enter into this class that will have an impact\n",
    "##check them all out!!\n",
    "# you can input a custom tokenizer as well\n",
    "bow = CountVectorizer(stopword='english')\n",
    "bow.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '00000000000',\n",
       " '0000000000000',\n",
       " '00442070001201',\n",
       " '00442071938940',\n",
       " '0092647',\n",
       " '01',\n",
       " '01quackeryrelatedtopics',\n",
       " '02',\n",
       " '03',\n",
       " '0385720270',\n",
       " '04',\n",
       " '0452278015',\n",
       " '0471383562',\n",
       " '05',\n",
       " '051010crat_atlarge',\n",
       " '0521592712',\n",
       " '0525934189',\n",
       " '0590025',\n",
       " '06',\n",
       " '0612100',\n",
       " '07',\n",
       " '07mm',\n",
       " '09',\n",
       " '09_401',\n",
       " '0s',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000103',\n",
       " '100gb',\n",
       " '100m',\n",
       " '100mhz',\n",
       " '102',\n",
       " '10260',\n",
       " '103',\n",
       " '103480',\n",
       " '104',\n",
       " '105',\n",
       " '10837',\n",
       " '10k',\n",
       " '10m',\n",
       " '10th',\n",
       " '10yo',\n",
       " '11',\n",
       " '113',\n",
       " '1136132459',\n",
       " '11519',\n",
       " '11698',\n",
       " '11th',\n",
       " '12',\n",
       " '124',\n",
       " '1286',\n",
       " '12oz',\n",
       " '12processo_eng',\n",
       " '13',\n",
       " '130',\n",
       " '130kw',\n",
       " '1318',\n",
       " '13299',\n",
       " '13328',\n",
       " '1367',\n",
       " '14',\n",
       " '14107',\n",
       " '144',\n",
       " '148',\n",
       " '14e2xpi',\n",
       " '14th',\n",
       " '15',\n",
       " '150',\n",
       " '150k',\n",
       " '1512',\n",
       " '15541',\n",
       " '1581345615',\n",
       " '15k',\n",
       " '15th',\n",
       " '16',\n",
       " '1600',\n",
       " '1605',\n",
       " '1628',\n",
       " '163',\n",
       " '165d',\n",
       " '165m',\n",
       " '1687',\n",
       " '16s',\n",
       " '17',\n",
       " '17259',\n",
       " '1751',\n",
       " '177',\n",
       " '17806',\n",
       " '17967',\n",
       " '18',\n",
       " '18157',\n",
       " '18528',\n",
       " '1880',\n",
       " '189cm',\n",
       " '19',\n",
       " '191',\n",
       " '1918944',\n",
       " '192',\n",
       " '1921',\n",
       " '19218',\n",
       " '1929',\n",
       " '1936',\n",
       " '1942',\n",
       " '19459',\n",
       " '1946',\n",
       " '1948',\n",
       " '1949',\n",
       " '1965',\n",
       " '1966',\n",
       " '1970',\n",
       " '1971',\n",
       " '1974',\n",
       " '1978',\n",
       " '1980s',\n",
       " '1981',\n",
       " '1982',\n",
       " '1984',\n",
       " '1986',\n",
       " '1987',\n",
       " '1989',\n",
       " '1990',\n",
       " '1991',\n",
       " '1993',\n",
       " '1994',\n",
       " '1995',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '1999',\n",
       " '19th',\n",
       " '1st',\n",
       " '1y',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '2001',\n",
       " '2002',\n",
       " '2003',\n",
       " '2004',\n",
       " '20041015134125',\n",
       " '2005',\n",
       " '20051212p2g00m0bu028000c',\n",
       " '2005_03_14',\n",
       " '2006',\n",
       " '20060112',\n",
       " '20060113',\n",
       " '2008',\n",
       " '200k',\n",
       " '2010',\n",
       " '2020',\n",
       " '20200',\n",
       " '2035',\n",
       " '2050',\n",
       " '207',\n",
       " '20722',\n",
       " '2074',\n",
       " '20766',\n",
       " '20economy',\n",
       " '20reddit',\n",
       " '20th',\n",
       " '20the',\n",
       " '21',\n",
       " '21466',\n",
       " '22',\n",
       " '22710',\n",
       " '22no',\n",
       " '22socialism',\n",
       " '23',\n",
       " '23041',\n",
       " '23112',\n",
       " '23538',\n",
       " '2357',\n",
       " '237',\n",
       " '24',\n",
       " '2411',\n",
       " '24385',\n",
       " '24560',\n",
       " '24756',\n",
       " '248',\n",
       " '249',\n",
       " '24carat',\n",
       " '24dot1',\n",
       " '25',\n",
       " '250',\n",
       " '250th',\n",
       " '25722',\n",
       " '25937',\n",
       " '25961',\n",
       " '25cc',\n",
       " '26',\n",
       " '260',\n",
       " '26495',\n",
       " '26599',\n",
       " '26615',\n",
       " '26764',\n",
       " '26959',\n",
       " '27',\n",
       " '27038',\n",
       " '27057',\n",
       " '27147',\n",
       " '27151',\n",
       " '2760',\n",
       " '27809',\n",
       " '2858',\n",
       " '29',\n",
       " '2b',\n",
       " '2c',\n",
       " '2cagent',\n",
       " '2castronaut',\n",
       " '2cc',\n",
       " '2cerlang',\n",
       " '2cfortran',\n",
       " '2chaskell',\n",
       " '2chtml',\n",
       " '2cjava',\n",
       " '2clisp',\n",
       " '2cperl',\n",
       " '2cphp',\n",
       " '2cpirate',\n",
       " '2cpython',\n",
       " '2crocket',\n",
       " '2cruby',\n",
       " '2csex',\n",
       " '2csmalltalk',\n",
       " '2csperm',\n",
       " '2cunderwater',\n",
       " '2curine',\n",
       " '2d',\n",
       " '2kw',\n",
       " '2liter',\n",
       " '2m',\n",
       " '2mm',\n",
       " '2nd',\n",
       " '2s',\n",
       " '2sf',\n",
       " '2y',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '30414093201',\n",
       " '32',\n",
       " '33',\n",
       " '331',\n",
       " '3339',\n",
       " '34',\n",
       " '342',\n",
       " '35',\n",
       " '350',\n",
       " '355',\n",
       " '36',\n",
       " '360',\n",
       " '36k',\n",
       " '37',\n",
       " '37103',\n",
       " '37signals',\n",
       " '38',\n",
       " '3db',\n",
       " '3dblended',\n",
       " '3g',\n",
       " '3rd',\n",
       " '3x4',\n",
       " '40',\n",
       " '400',\n",
       " '4000th',\n",
       " '4015',\n",
       " '403',\n",
       " '404',\n",
       " '41',\n",
       " '41505',\n",
       " '42',\n",
       " '4202741',\n",
       " '42363',\n",
       " '43',\n",
       " '4326',\n",
       " '44',\n",
       " '444',\n",
       " '44cc',\n",
       " '45',\n",
       " '453',\n",
       " '455',\n",
       " '458',\n",
       " '4598714',\n",
       " '45s',\n",
       " '46',\n",
       " '47',\n",
       " '48',\n",
       " '49',\n",
       " '4d257aac09496b6d',\n",
       " '4q',\n",
       " '4th',\n",
       " '4ths',\n",
       " '4x',\n",
       " '50',\n",
       " '500',\n",
       " '507846',\n",
       " '51',\n",
       " '512000000000000',\n",
       " '52',\n",
       " '53',\n",
       " '5385434',\n",
       " '54238',\n",
       " '5529811',\n",
       " '56',\n",
       " '5648',\n",
       " '567',\n",
       " '57',\n",
       " '5713383956',\n",
       " '58',\n",
       " '59',\n",
       " '591',\n",
       " '598einst',\n",
       " '5b',\n",
       " '5d',\n",
       " '5fc3l4jqjvuj',\n",
       " '5fencoding',\n",
       " '5kw',\n",
       " '5x',\n",
       " '5y',\n",
       " '60',\n",
       " '600',\n",
       " '6000',\n",
       " '60k',\n",
       " '61091',\n",
       " '63',\n",
       " '630',\n",
       " '637',\n",
       " '64',\n",
       " '6475319',\n",
       " '66',\n",
       " '661',\n",
       " '67248daf452325d0',\n",
       " '68k',\n",
       " '69946',\n",
       " '6v6gt',\n",
       " '70',\n",
       " '7000',\n",
       " '70s',\n",
       " '72',\n",
       " '737',\n",
       " '747',\n",
       " '75',\n",
       " '757',\n",
       " '76',\n",
       " '76ers',\n",
       " '7750701',\n",
       " '7780',\n",
       " '78',\n",
       " '7th',\n",
       " '80',\n",
       " '8000',\n",
       " '8088',\n",
       " '8090',\n",
       " '80mb',\n",
       " '80s',\n",
       " '81',\n",
       " '8123',\n",
       " '85',\n",
       " '86',\n",
       " '8688',\n",
       " '87',\n",
       " '88',\n",
       " '8818454',\n",
       " '888',\n",
       " '89',\n",
       " '89q3',\n",
       " '8hrs',\n",
       " '8note57',\n",
       " '8oz',\n",
       " '8th',\n",
       " '90',\n",
       " '900',\n",
       " '900584',\n",
       " '90s',\n",
       " '911research',\n",
       " '9129',\n",
       " '93',\n",
       " '933262154439',\n",
       " '93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000',\n",
       " '940',\n",
       " '948',\n",
       " '95',\n",
       " '96',\n",
       " '961001',\n",
       " '97',\n",
       " '97mojo_400',\n",
       " '98',\n",
       " '99',\n",
       " '996',\n",
       " '9kwh',\n",
       " '9za',\n",
       " '______',\n",
       " '_a',\n",
       " '_absolutely_',\n",
       " '_again',\n",
       " '_all',\n",
       " '_atheist_',\n",
       " '_belief_',\n",
       " '_believe_',\n",
       " '_best_',\n",
       " '_billion_',\n",
       " '_bit_',\n",
       " '_capitol',\n",
       " '_chatterjee',\n",
       " '_christians_',\n",
       " '_could_',\n",
       " '_decrease_',\n",
       " '_dis_proven',\n",
       " '_discussed_',\n",
       " '_explain_',\n",
       " '_for_',\n",
       " '_fox',\n",
       " '_global_',\n",
       " '_highly_',\n",
       " '_how',\n",
       " '_i',\n",
       " '_in',\n",
       " '_incredibly_',\n",
       " '_independent_',\n",
       " '_iraq_',\n",
       " '_is_',\n",
       " '_know_',\n",
       " '_list',\n",
       " '_long',\n",
       " '_maarten',\n",
       " '_many',\n",
       " '_mathematica_',\n",
       " '_more_',\n",
       " '_most',\n",
       " '_mother',\n",
       " '_next_',\n",
       " '_nobody',\n",
       " '_not_',\n",
       " '_now_',\n",
       " '_okay',\n",
       " '_one',\n",
       " '_other',\n",
       " '_other_',\n",
       " '_overwhelmingly_',\n",
       " '_people',\n",
       " '_persistent_',\n",
       " '_really',\n",
       " '_really_',\n",
       " '_religious_',\n",
       " '_same_',\n",
       " '_scientifically_',\n",
       " '_should_',\n",
       " '_so',\n",
       " '_the',\n",
       " '_their_',\n",
       " '_this_',\n",
       " '_totally_',\n",
       " '_verify_',\n",
       " '_why_',\n",
       " '_wille',\n",
       " '_with',\n",
       " '_you',\n",
       " '_you_',\n",
       " 'a3',\n",
       " 'a4',\n",
       " 'a_bayes',\n",
       " 'aaaaaaaaaaaaaarrrrrgh',\n",
       " 'aac',\n",
       " 'aacs',\n",
       " 'aagh',\n",
       " 'aapl',\n",
       " 'aaron',\n",
       " 'aaronkoblin',\n",
       " 'aaronsw',\n",
       " 'aasted',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandoning',\n",
       " 'abbie',\n",
       " 'abbie_hoffman',\n",
       " 'abc',\n",
       " 'abcl',\n",
       " 'aberrations',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'abiogenisis',\n",
       " 'abismal',\n",
       " 'able',\n",
       " 'ablity',\n",
       " 'abnoxious',\n",
       " 'abominable',\n",
       " 'abomination',\n",
       " 'abortion',\n",
       " 'about',\n",
       " 'about_cisco_country_students_and_grads_list',\n",
       " 'above',\n",
       " 'abraham',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absolutist',\n",
       " 'absorb',\n",
       " 'absorbed',\n",
       " 'absorber',\n",
       " 'abstract',\n",
       " 'abstraction',\n",
       " 'abstractions',\n",
       " 'abstractnonsense',\n",
       " 'absurd',\n",
       " 'absurdity',\n",
       " 'abu',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abuses',\n",
       " 'abusing',\n",
       " 'abusive',\n",
       " 'ac',\n",
       " 'ac40',\n",
       " 'academia',\n",
       " 'academic',\n",
       " 'academically',\n",
       " 'acc',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'access',\n",
       " 'accessibility',\n",
       " 'accessible',\n",
       " 'accessories',\n",
       " 'accessory',\n",
       " 'accgen',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidentally',\n",
       " 'accidently',\n",
       " 'accidents',\n",
       " 'acclaimed',\n",
       " 'accomodate',\n",
       " 'accompany',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishing',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'accordingto',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'accountable',\n",
       " 'accountants',\n",
       " 'accounting',\n",
       " 'accounts',\n",
       " 'accrues',\n",
       " 'accumulated',\n",
       " 'accumulator',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accusation',\n",
       " 'accuse',\n",
       " 'accused',\n",
       " 'accuses',\n",
       " 'accusing',\n",
       " 'acd',\n",
       " 'acedemia',\n",
       " 'acentrism',\n",
       " 'acentrists',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'acholholic',\n",
       " 'acid',\n",
       " 'acidic',\n",
       " 'acids',\n",
       " 'ack',\n",
       " 'acknowledge',\n",
       " 'acl',\n",
       " 'aclu',\n",
       " 'acolytes',\n",
       " 'acquaintances',\n",
       " 'acquired',\n",
       " 'acres',\n",
       " 'acrobat',\n",
       " 'across',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'activation',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actuality',\n",
       " 'actually',\n",
       " 'acually',\n",
       " 'acute',\n",
       " 'ad',\n",
       " 'ada',\n",
       " 'adademic',\n",
       " 'adam',\n",
       " 'adamantly',\n",
       " 'adapt',\n",
       " 'adaptation',\n",
       " 'adaptations',\n",
       " 'adapted',\n",
       " 'adapters',\n",
       " 'adapting',\n",
       " 'adaption',\n",
       " 'adaquate',\n",
       " 'adblock',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addicted',\n",
       " 'addicting',\n",
       " 'addictions',\n",
       " 'addicts',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additionally',\n",
       " 'additions',\n",
       " 'additive',\n",
       " 'address',\n",
       " 'addressable',\n",
       " 'addressed',\n",
       " 'addresses',\n",
       " 'addressing',\n",
       " 'adds',\n",
       " 'adele',\n",
       " 'adept',\n",
       " 'adequately',\n",
       " 'adhere',\n",
       " 'adherence',\n",
       " 'adherent',\n",
       " 'adherents',\n",
       " 'adjecent',\n",
       " 'adjective',\n",
       " 'adjectives',\n",
       " 'adjust',\n",
       " 'adjusting',\n",
       " 'adjusts',\n",
       " 'adlab',\n",
       " 'admin',\n",
       " 'administer',\n",
       " 'administration',\n",
       " 'administrations',\n",
       " 'administrative',\n",
       " 'administrator',\n",
       " 'admire',\n",
       " 'admissible',\n",
       " 'admissions',\n",
       " 'admit',\n",
       " 'admits',\n",
       " 'admitted',\n",
       " 'admittedly',\n",
       " 'adnam',\n",
       " 'adobe',\n",
       " 'adopt',\n",
       " 'adopters',\n",
       " 'adopting',\n",
       " 'adoption',\n",
       " 'adorable',\n",
       " 'adoration',\n",
       " 'ads',\n",
       " 'adsb370itsju',\n",
       " 'adsense',\n",
       " 'adsl',\n",
       " 'adult',\n",
       " 'adultery',\n",
       " 'adults',\n",
       " 'adulturant',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advances',\n",
       " 'advantage',\n",
       " 'advantages',\n",
       " 'adventures',\n",
       " 'advert',\n",
       " 'advertised',\n",
       " 'advertiser',\n",
       " 'advertisers',\n",
       " 'advertising',\n",
       " 'advertorial',\n",
       " 'advice',\n",
       " 'advised',\n",
       " 'adviser',\n",
       " 'advisors',\n",
       " 'advocacy',\n",
       " 'advocating',\n",
       " 'adwords',\n",
       " 'aec',\n",
       " 'aesthetic',\n",
       " 'afaics',\n",
       " 'afaik',\n",
       " 'affable',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affective',\n",
       " 'affects',\n",
       " 'affero',\n",
       " 'afficianados',\n",
       " 'affix',\n",
       " 'afford',\n",
       " 'affronts',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'africans',\n",
       " 'afsyuhud',\n",
       " 'after',\n",
       " 'afterlife',\n",
       " 'afternoon',\n",
       " 'afterward',\n",
       " 'afv',\n",
       " 'again',\n",
       " 'against',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'ageless',\n",
       " 'agencies',\n",
       " 'agency',\n",
       " 'agendas',\n",
       " 'agentive',\n",
       " 'agents',\n",
       " 'ages',\n",
       " 'aggregators',\n",
       " 'agile',\n",
       " 'agilemap',\n",
       " 'aging',\n",
       " 'agnostic',\n",
       " 'agnosticism',\n",
       " 'agnostics',\n",
       " 'ago',\n",
       " 'agony',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'agreeing',\n",
       " 'agreement',\n",
       " 'agrees',\n",
       " 'agriculture',\n",
       " 'ah',\n",
       " 'ahead',\n",
       " 'ahhhh',\n",
       " 'ahmadinejad',\n",
       " 'ai',\n",
       " 'aid',\n",
       " 'aide',\n",
       " 'aids',\n",
       " 'aim',\n",
       " 'aimed',\n",
       " 'aiming',\n",
       " 'aims',\n",
       " 'ain',\n",
       " 'air',\n",
       " 'aircraft',\n",
       " 'aired',\n",
       " 'airline',\n",
       " 'airplane',\n",
       " 'airplanes',\n",
       " 'airport',\n",
       " 'airports',\n",
       " 'airs',\n",
       " 'airstrike',\n",
       " 'ajax',\n",
       " 'aka',\n",
       " 'ake',\n",
       " 'akhenaton',\n",
       " 'akin',\n",
       " 'al',\n",
       " 'alamat',\n",
       " 'alan',\n",
       " 'alarm',\n",
       " 'alarming',\n",
       " 'alarmists',\n",
       " 'alas',\n",
       " 'alaska',\n",
       " 'albania',\n",
       " 'albanian',\n",
       " 'albanians',\n",
       " 'albatross',\n",
       " 'albeit',\n",
       " 'albert',\n",
       " 'albert_hofmann',\n",
       " 'album',\n",
       " 'albums',\n",
       " 'alcohol',\n",
       " 'alcoholic',\n",
       " 'alertnet',\n",
       " 'alerts',\n",
       " 'alex',\n",
       " 'alexa',\n",
       " 'alexis',\n",
       " 'alg',\n",
       " 'algebra',\n",
       " 'algorithims',\n",
       " 'algorithm',\n",
       " 'algorithmical',\n",
       " 'algorithms',\n",
       " 'ali',\n",
       " 'alice',\n",
       " 'aliens',\n",
       " 'align',\n",
       " 'aligned',\n",
       " 'aligns',\n",
       " 'alike',\n",
       " 'alito',\n",
       " 'alittle',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allah',\n",
       " 'allegations',\n",
       " 'alleged',\n",
       " 'allegiance',\n",
       " 'allegory',\n",
       " 'allen',\n",
       " 'alleviate',\n",
       " 'alleviation',\n",
       " 'allocation',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'allready',\n",
       " 'allso',\n",
       " 'alluded',\n",
       " 'alluding',\n",
       " 'allways',\n",
       " 'ally',\n",
       " 'almost',\n",
       " 'almostly',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alongside',\n",
       " 'alot',\n",
       " 'aloud',\n",
       " 'alper',\n",
       " 'alpha',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'alt',\n",
       " 'alternate',\n",
       " 'alternately',\n",
       " 'alternative',\n",
       " 'alternatives',\n",
       " 'although',\n",
       " 'althought',\n",
       " 'altitude',\n",
       " 'altogether',\n",
       " 'altough',\n",
       " 'alumnium',\n",
       " 'always',\n",
       " 'am',\n",
       " 'amartya',\n",
       " 'amateur',\n",
       " 'amazed',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'amazon',\n",
       " 'ambient',\n",
       " 'ambigious',\n",
       " 'ambiguity',\n",
       " 'ambition',\n",
       " 'ambitions',\n",
       " 'ambitious',\n",
       " 'amconmag',\n",
       " 'amd',\n",
       " 'amen',\n",
       " 'amendment',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americandialect',\n",
       " 'americans',\n",
       " 'americas',\n",
       " 'amerika',\n",
       " 'amethyst',\n",
       " 'amino',\n",
       " 'ammo',\n",
       " 'ammount',\n",
       " 'ammounts',\n",
       " 'amnesty',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amoral',\n",
       " 'amoung',\n",
       " 'amount',\n",
       " 'amount2',\n",
       " 'amounted',\n",
       " 'amounts',\n",
       " 'amp',\n",
       " 'ampersand',\n",
       " 'amphibian',\n",
       " 'amulet',\n",
       " 'amurikins',\n",
       " 'amused',\n",
       " 'amusing',\n",
       " 'amusingly',\n",
       " 'an',\n",
       " 'anal',\n",
       " 'analog',\n",
       " 'analogies',\n",
       " 'analogy',\n",
       " 'analyse',\n",
       " 'analyses',\n",
       " 'analysis',\n",
       " 'analyst',\n",
       " 'analysts',\n",
       " 'analyzed',\n",
       " 'analyzes',\n",
       " 'anandtech',\n",
       " 'anarchist',\n",
       " 'anarchy',\n",
       " 'ancestors',\n",
       " 'anchor',\n",
       " 'ancient',\n",
       " 'and',\n",
       " 'anderson',\n",
       " 'anecdoctal',\n",
       " 'anecdotal',\n",
       " 'anecdotes',\n",
       " 'angeles',\n",
       " 'angels',\n",
       " 'anger',\n",
       " 'angle',\n",
       " 'angled',\n",
       " 'angles',\n",
       " 'anglican',\n",
       " 'angry',\n",
       " 'angst',\n",
       " 'anic',\n",
       " 'animal',\n",
       " 'animalia',\n",
       " 'animals',\n",
       " 'animation',\n",
       " 'animations',\n",
       " 'anit',\n",
       " 'anna',\n",
       " 'annihilation',\n",
       " 'annointing',\n",
       " 'annotations',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'announces',\n",
       " 'announcing',\n",
       " 'annoyable',\n",
       " 'annoyance',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'annoys',\n",
       " 'annual',\n",
       " 'annually',\n",
       " 'anointest',\n",
       " 'anonymity',\n",
       " 'anonymizer',\n",
       " 'anonymous',\n",
       " 'anonymous_hater',\n",
       " 'another',\n",
       " 'anothers',\n",
       " 'ansi',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answering',\n",
       " 'answers',\n",
       " 'antagonizing',\n",
       " 'antenna',\n",
       " 'anthiest',\n",
       " 'anthing',\n",
       " 'anthology',\n",
       " 'anthony',\n",
       " 'anthropology',\n",
       " 'anthropomorphized',\n",
       " 'anti',\n",
       " 'anticipates',\n",
       " 'anticipating',\n",
       " 'antimatter',\n",
       " 'antispy',\n",
       " 'antivirus',\n",
       " 'antiwar',\n",
       " 'antonym',\n",
       " 'anxiety',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anythin',\n",
       " 'anything',\n",
       " 'anytime',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'aol',\n",
       " 'aolserver',\n",
       " 'aom',\n",
       " 'ap',\n",
       " 'ap_on_go_pr_wh',\n",
       " ...]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## let's look at out our features\n",
    "bow.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'early': 3977,\n",
       " '2006': 146,\n",
       " 'probable': 9336,\n",
       " 'date': 3221,\n",
       " 'if': 6030,\n",
       " 'you': 13310,\n",
       " 'are': 1088,\n",
       " 'going': 5355,\n",
       " 'to': 12143,\n",
       " 'post': 9140,\n",
       " 'something': 11133,\n",
       " 'that': 11983,\n",
       " 'has': 5622,\n",
       " 'link': 7125,\n",
       " 'the': 11986,\n",
       " 'original': 8486,\n",
       " 'author': 1291,\n",
       " 'why': 13060,\n",
       " 'not': 8209,\n",
       " 'just': 6758,\n",
       " 'instead': 6373,\n",
       " 'of': 8336,\n",
       " 'someone': 11130,\n",
       " 'copy': 2918,\n",
       " 'microsoft': 7643,\n",
       " 'hates': 5631,\n",
       " 'it': 6603,\n",
       " 'own': 8588,\n",
       " 'products': 9369,\n",
       " 'who': 13044,\n",
       " 'knew': 6852,\n",
       " 'this': 12055,\n",
       " 'looks': 7222,\n",
       " 'interesting': 6431,\n",
       " 'but': 1974,\n",
       " 'already': 840,\n",
       " 'aired': 758,\n",
       " 'and': 918,\n",
       " 'like': 7101,\n",
       " 'there': 12025,\n",
       " 'streaming': 11482,\n",
       " 'video': 12770,\n",
       " 'so': 11078,\n",
       " 'what': 13007,\n",
       " 'point': 9037,\n",
       " 'have': 5634,\n",
       " 'nothing': 8220,\n",
       " 'good': 5363,\n",
       " 'things': 12044,\n",
       " 'say': 10474,\n",
       " 'about': 490,\n",
       " 'dell': 3359,\n",
       " 'tech': 11879,\n",
       " 'support': 11678,\n",
       " 'many': 7407,\n",
       " 'time': 12112,\n",
       " 've': 12709,\n",
       " 'called': 2031,\n",
       " 'in': 6142,\n",
       " 'faulty': 4682,\n",
       " 'part': 8676,\n",
       " 'had': 5550,\n",
       " 'replacement': 10042,\n",
       " 'at': 1226,\n",
       " 'front': 5095,\n",
       " 'door': 3825,\n",
       " 'two': 12381,\n",
       " 'days': 3235,\n",
       " 'with': 13133,\n",
       " 'box': 1798,\n",
       " 'ship': 10786,\n",
       " 'old': 8372,\n",
       " 'one': 8386,\n",
       " 'back': 1347,\n",
       " 'first': 4824,\n",
       " 'class': 2407,\n",
       " 'service': 10700,\n",
       " 'ask': 1167,\n",
       " 'me': 7513,\n",
       " 'by': 1990,\n",
       " 'mean': 7515,\n",
       " 'employees': 4157,\n",
       " 'out': 8520,\n",
       " '60': 321,\n",
       " '000': 1,\n",
       " 'or': 8449,\n",
       " 'is': 6571,\n",
       " 'exactly': 4416,\n",
       " 'same': 10438,\n",
       " 'behavior': 1512,\n",
       " 'as': 1153,\n",
       " 'very': 12750,\n",
       " 'lisp': 7139,\n",
       " 'dialects': 3531,\n",
       " 'they': 12036,\n",
       " 'dynamic': 3966,\n",
       " 'scoping': 10539,\n",
       " 'interpreter': 6452,\n",
       " 'lexical': 7063,\n",
       " 'wonder': 13159,\n",
       " 'history': 5792,\n",
       " 'deleted': 3348,\n",
       " 'because': 1476,\n",
       " 'different': 3564,\n",
       " 'better': 1575,\n",
       " 'here': 5723,\n",
       " 'your': 13314,\n",
       " 'sign': 10877,\n",
       " 'ok': 8368,\n",
       " 'looking': 7221,\n",
       " 'forward': 5007,\n",
       " 'reaction': 9749,\n",
       " 'from': 5094,\n",
       " 'id': 6000,\n",
       " 'crowd': 3092,\n",
       " 'succeeds': 11596,\n",
       " 'hope': 5876,\n",
       " 'politically': 9064,\n",
       " 'motivated': 7871,\n",
       " 'annoys': 952,\n",
       " 'world': 13192,\n",
       " 'seems': 10626,\n",
       " 'less': 7043,\n",
       " 'safe': 10412,\n",
       " 'now': 8237,\n",
       " 'than': 11976,\n",
       " 'did': 3552,\n",
       " 'years': 13292,\n",
       " 'ago': 735,\n",
       " 'we': 12945,\n",
       " 'helped': 5712,\n",
       " 'anything': 990,\n",
       " 'way': 12942,\n",
       " 'behaved': 1510,\n",
       " 'created': 3031,\n",
       " 'enemies': 4204,\n",
       " 'people': 8791,\n",
       " 'previously': 9282,\n",
       " 'didn': 3554,\n",
       " 'sway': 11738,\n",
       " 'strongly': 11512,\n",
       " 'either': 4079,\n",
       " 'opinionnaire': 8420,\n",
       " 'doesn': 3789,\n",
       " 'appear': 1025,\n",
       " 'write': 13225,\n",
       " 'name': 7987,\n",
       " 'on': 8382,\n",
       " 'talking': 11826,\n",
       " 'points': 9043,\n",
       " 'thinking': 12048,\n",
       " 'quite': 9630,\n",
       " 'english': 4223,\n",
       " 'does': 3788,\n",
       " 'seem': 10623,\n",
       " 'an': 900,\n",
       " 'odd': 8328,\n",
       " 'choice': 2337,\n",
       " 'awesome': 1333,\n",
       " 'caucho': 2163,\n",
       " 'means': 7521,\n",
       " 'step': 11411,\n",
       " 'update': 12600,\n",
       " 'breaks': 1839,\n",
       " 'all': 811,\n",
       " 'code': 2496,\n",
       " 'can': 2050,\n",
       " 'see': 10616,\n",
       " 'changes': 2251,\n",
       " 'changelog': 2249,\n",
       " 'much': 7914,\n",
       " 'fun': 5123,\n",
       " 'money': 7811,\n",
       " 'isn': 6586,\n",
       " 'making': 7365,\n",
       " 'anyone': 988,\n",
       " 'happy': 5599,\n",
       " 'will': 13089,\n",
       " 'gladly': 5311,\n",
       " 'accept': 518,\n",
       " 'my': 7960,\n",
       " 'paypal': 8751,\n",
       " 'account': 543,\n",
       " 'great': 5444,\n",
       " 'fonts': 4928,\n",
       " 'interpretation': 6449,\n",
       " 'article': 1137,\n",
       " 'was': 12914,\n",
       " 'more': 7842,\n",
       " 'singleton': 10929,\n",
       " 'pattern': 8736,\n",
       " 'used': 12636,\n",
       " 'crutch': 3107,\n",
       " 'programmers': 9401,\n",
       " 'be': 1455,\n",
       " 'able': 484,\n",
       " 'claim': 2397,\n",
       " 'understand': 12468,\n",
       " 'object': 8278,\n",
       " 'oriented': 8483,\n",
       " 'programming': 9403,\n",
       " 'design': 3449,\n",
       " 'patterns': 8737,\n",
       " 'seen': 10627,\n",
       " 'times': 12116,\n",
       " 'myself': 7963,\n",
       " 'when': 13017,\n",
       " 'interviewing': 6473,\n",
       " 'also': 842,\n",
       " 'lot': 7243,\n",
       " 'where': 13019,\n",
       " 'describes': 3439,\n",
       " 'often': 8359,\n",
       " 'places': 8977,\n",
       " 'completely': 2660,\n",
       " 'unnecessary': 12547,\n",
       " 'stupid': 11542,\n",
       " 'per': 8796,\n",
       " 'se': 10581,\n",
       " 'become': 1477,\n",
       " 'most': 7863,\n",
       " 'obvious': 8310,\n",
       " 'symbol': 11764,\n",
       " 'how': 5912,\n",
       " 'misunderstand': 7743,\n",
       " 'misuse': 7747,\n",
       " 'general': 5240,\n",
       " 'lush': 7291,\n",
       " 'could': 2963,\n",
       " 'wonderful': 13161,\n",
       " 'for': 4949,\n",
       " 'actual': 594,\n",
       " 'application': 1039,\n",
       " 'several': 10715,\n",
       " 'months': 7831,\n",
       " 'almost': 831,\n",
       " 'undocumented': 12479,\n",
       " 'sad': 10404,\n",
       " 'found': 5018,\n",
       " 'really': 9775,\n",
       " 'potential': 9158,\n",
       " 'some': 11126,\n",
       " 'others': 8510,\n",
       " 'definitely': 3332,\n",
       " 'nonsense': 8188,\n",
       " 'end': 4189,\n",
       " 'don': 3811,\n",
       " 'think': 12045,\n",
       " 'worth': 13207,\n",
       " 'reading': 9760,\n",
       " 'typical': 12391,\n",
       " 'vapid': 12687,\n",
       " 'year': 13287,\n",
       " 'no': 8163,\n",
       " 'news': 8114,\n",
       " 'report': 10048,\n",
       " 'story': 11465,\n",
       " 'wish': 13126,\n",
       " 'tag': 11811,\n",
       " 'these': 12034,\n",
       " 'assiduously': 1193,\n",
       " 'avoid': 1322,\n",
       " 'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz': 13353,\n",
       " 'must': 7951,\n",
       " 'read': 9752,\n",
       " 'funny': 5143,\n",
       " 'superficial': 11661,\n",
       " 'crashed': 3023,\n",
       " 'browser': 1889,\n",
       " 'limited': 7115,\n",
       " 'use': 12635,\n",
       " 'would': 13211,\n",
       " 'want': 12891,\n",
       " 'search': 10585,\n",
       " 'description': 3442,\n",
       " 'comments': 2592,\n",
       " 'documentation': 3779,\n",
       " 'source': 11176,\n",
       " 'itself': 6615,\n",
       " 'amartya': 856,\n",
       " 'sen': 10656,\n",
       " 'development': 3507,\n",
       " 'freedom': 5058,\n",
       " 'http': 5920,\n",
       " 'www': 13246,\n",
       " 'amazon': 861,\n",
       " 'com': 2553,\n",
       " 'gp': 5390,\n",
       " 'product': 9364,\n",
       " '0385720270': 11,\n",
       " 'qid': 9593,\n",
       " '1136132459': 47,\n",
       " 'sr': 11318,\n",
       " 'ref': 9870,\n",
       " 'pd_bbs_1': 8758,\n",
       " '102': 34,\n",
       " '5529811': 304,\n",
       " '0612100': 21,\n",
       " '507846': 297,\n",
       " 'amp': 892,\n",
       " 'books': 1748,\n",
       " 'glance': 5313,\n",
       " 'takes': 11819,\n",
       " 'idea': 6002,\n",
       " 'applies': 1042,\n",
       " 'developing': 3506,\n",
       " 'countries': 2976,\n",
       " 'book': 1742,\n",
       " 'surface': 11700,\n",
       " 'animation': 936,\n",
       " 'colour': 2548,\n",
       " 'dig': 3571,\n",
       " 'deeper': 3309,\n",
       " 'off': 8337,\n",
       " 'wasteful': 12924,\n",
       " 'incompatible': 6173,\n",
       " 'flash': 4857,\n",
       " 'ignorant': 6034,\n",
       " 'its': 6614,\n",
       " 'web': 12959,\n",
       " 'context': 2835,\n",
       " 'cute': 3165,\n",
       " 'interface': 6434,\n",
       " 'stand': 11348,\n",
       " 'alone': 833,\n",
       " 'poor': 9091,\n",
       " 'removed': 10011,\n",
       " 'work': 13180,\n",
       " 'home': 5839,\n",
       " 'important': 6113,\n",
       " 'set': 10706,\n",
       " 'boundaries': 1790,\n",
       " 'other': 8506,\n",
       " 'creatures': 3043,\n",
       " 'should': 10828,\n",
       " 'probably': 9337,\n",
       " 'closes': 2459,\n",
       " 'need': 8045,\n",
       " 'agree': 737,\n",
       " 'may': 7497,\n",
       " 'likely': 7104,\n",
       " 'make': 7359,\n",
       " 'demands': 3368,\n",
       " 'worker': 13184,\n",
       " 'up': 12599,\n",
       " 'train': 12229,\n",
       " 'them': 12002,\n",
       " 'examples': 4422,\n",
       " 'lame': 6913,\n",
       " 'workers': 13185,\n",
       " 'haven': 5635,\n",
       " 'done': 3816,\n",
       " 'guy': 5533,\n",
       " 'driving': 3904,\n",
       " 'his': 5780,\n",
       " 'neighbour': 8066,\n",
       " 'airport': 762,\n",
       " 'he': 5645,\n",
       " 'her': 5721,\n",
       " 'number': 8252,\n",
       " 'random': 9691,\n",
       " 'day': 3233,\n",
       " 'next': 8128,\n",
       " 'week': 12975,\n",
       " 'phone': 8893,\n",
       " 'pick': 8924,\n",
       " 'him': 5766,\n",
       " 'groceries': 5472,\n",
       " 'click': 2435,\n",
       " 'understanding': 12471,\n",
       " 'another': 960,\n",
       " 'hack': 5543,\n",
       " 'getting': 5278,\n",
       " 'around': 1116,\n",
       " 'magnetic': 7334,\n",
       " 'force': 4954,\n",
       " 'field': 4759,\n",
       " 'those': 12060,\n",
       " 'tricking': 12291,\n",
       " 'yourself': 13316,\n",
       " 'hacks': 5548,\n",
       " 'tell': 11906,\n",
       " '15': 69,\n",
       " 'minutes': 7697,\n",
       " 'do': 3768,\n",
       " 'gather': 5207,\n",
       " 'timer': 12115,\n",
       " 'begin': 1501,\n",
       " 'chances': 2246,\n",
       " 'goes': 5353,\n",
       " 'll': 7171,\n",
       " 'overcome': 8559,\n",
       " 'engaged': 4214,\n",
       " 'project': 9414,\n",
       " 'ready': 9762,\n",
       " 'keep': 6785,\n",
       " 'working': 13187,\n",
       " 'schedule': 10504,\n",
       " 'minute': 7696,\n",
       " 'session': 10703,\n",
       " 'later': 6950,\n",
       " 'breaking': 1838,\n",
       " 'down': 3842,\n",
       " 'task': 11849,\n",
       " 'well': 12991,\n",
       " 'paul': 8739,\n",
       " 'been': 1487,\n",
       " 'stuff': 11534,\n",
       " 'totally': 12195,\n",
       " 'enjoy': 4231,\n",
       " 'knack': 6850,\n",
       " 'identifying': 6011,\n",
       " 'heart': 5673,\n",
       " 'issue': 6599,\n",
       " 'skilled': 10967,\n",
       " 'finding': 4800,\n",
       " 'analogies': 903,\n",
       " 'explaining': 4511,\n",
       " 'thanks': 11981,\n",
       " 'inspiration': 6356,\n",
       " 'john': 6691,\n",
       " 'rewritten': 10216,\n",
       " 'dojo': 3798,\n",
       " 'alper': 838,\n",
       " 'nl': 8162,\n",
       " 'blog': 1675,\n",
       " '72': 340,\n",
       " 'except': 4432,\n",
       " 'which': 13025,\n",
       " 'filler': 4781,\n",
       " 'sort': 11160,\n",
       " 'calling': 2032,\n",
       " 'synthetic': 11785,\n",
       " 'life': 7084,\n",
       " 'form': 4979,\n",
       " 'reach': 9744,\n",
       " 're': 9743,\n",
       " 'manipulating': 7392,\n",
       " 'genetics': 5257,\n",
       " 'using': 12646,\n",
       " 'scary': 10498,\n",
       " 'new': 8104,\n",
       " 'breeding': 1848,\n",
       " 'such': 11603,\n",
       " 'quaint': 9596,\n",
       " 'mythology': 7975,\n",
       " 'wouldn': 13212,\n",
       " 'considered': 2782,\n",
       " 'then': 12007,\n",
       " 'kind': 6828,\n",
       " 'trying': 12337,\n",
       " 'attract': 1268,\n",
       " 'perhaps': 8814,\n",
       " 'dumb': 3937,\n",
       " 'acknowledge': 572,\n",
       " 'powerful': 9173,\n",
       " 'tools': 12177,\n",
       " 'elegant': 4092,\n",
       " 'hiring': 5779,\n",
       " 'rightly': 10251,\n",
       " 'get': 5276,\n",
       " 'pissed': 8963,\n",
       " 'essay': 4339,\n",
       " 'wow': 13217,\n",
       " 'cool': 2903,\n",
       " 'linked': 7126,\n",
       " 'page': 8613,\n",
       " 'heavy': 5687,\n",
       " 'css': 3116,\n",
       " 'though': 12062,\n",
       " 'common': 2603,\n",
       " 'sense': 10667,\n",
       " 'dickhead': 3545,\n",
       " 'any': 984,\n",
       " 'rules': 10370,\n",
       " 'anywhere': 994,\n",
       " 'none': 8182,\n",
       " 'take': 11817,\n",
       " 'logic': 7200,\n",
       " 'explanation': 4513,\n",
       " 'statement': 11379,\n",
       " 'gt': 5496,\n",
       " 'russians': 10389,\n",
       " 'hand': 5573,\n",
       " 'everything': 4400,\n",
       " 'stay': 11398,\n",
       " 'sober': 11081,\n",
       " 'while': 13026,\n",
       " 'drinking': 3895,\n",
       " 'alcohol': 788,\n",
       " 'possible': 9138,\n",
       " 'question': 9616,\n",
       " 'stands': 11356,\n",
       " 'alcoholic': 789,\n",
       " 'drinks': 3896,\n",
       " 'tasting': 11855,\n",
       " 'certain': 2219,\n",
       " 'wines': 13108,\n",
       " 'drink': 3893,\n",
       " 'written': 13233,\n",
       " 'true': 12320,\n",
       " 'note': 8212,\n",
       " 'mostly': 7864,\n",
       " 'wives': 13143,\n",
       " 'children': 2322,\n",
       " 'key': 6801,\n",
       " 'effectively': 4053,\n",
       " 'combination': 2557,\n",
       " 'highly': 5756,\n",
       " 'disciplined': 3637,\n",
       " 'isolated': 6590,\n",
       " 'usable': 12631,\n",
       " 'area': 1089,\n",
       " 'latter': 6956,\n",
       " 'even': 4390,\n",
       " 'office': 8349,\n",
       " 'sensible': 10669,\n",
       " 'periods': 8818,\n",
       " 'without': 13137,\n",
       " 'interruptions': 6459,\n",
       " 'effectiveness': 4054,\n",
       " 'drops': 3909,\n",
       " 'nil': 8148,\n",
       " 'similarly': 10899,\n",
       " 'movies': 7899,\n",
       " 'forever': 4969,\n",
       " 'download': 3845,\n",
       " 'come': 2568,\n",
       " 'pretty': 9275,\n",
       " 'shite': 10793,\n",
       " 'actually': 596,\n",
       " 'generally': 5244,\n",
       " 'look': 7219,\n",
       " 'fantastic': 4648,\n",
       " 'only': 8392,\n",
       " 'night': 8142,\n",
       " 'least': 7007,\n",
       " 'friend': 5083,\n",
       " 'tells': 11909,\n",
       " 'buys': 1985,\n",
       " 'into': 6477,\n",
       " 'sifr': 10874,\n",
       " 'belongs': 1538,\n",
       " 'list': 7143,\n",
       " 'websites': 12970,\n",
       " 'god': 5344,\n",
       " 'everyone': 4399,\n",
       " 'ignored': 6036,\n",
       " '2005': 143,\n",
       " 'aasted': 471,\n",
       " 'org': 8468,\n",
       " 'adblock': 612,\n",
       " 'viewtopic': 12785,\n",
       " 'php': 8911,\n",
       " '1687': 84,\n",
       " 'reason': 9782,\n",
       " 'music': 7947,\n",
       " 'industry': 6246,\n",
       " 'loosing': 7232,\n",
       " 'focus': 4903,\n",
       " 'disposable': 3693,\n",
       " 'find': 4798,\n",
       " 'men': 7577,\n",
       " 'women': 13157,\n",
       " 'sing': 10923,\n",
       " 'made': 7324,\n",
       " 'know': 6859,\n",
       " 'sound': 11169,\n",
       " 'moment': 7802,\n",
       " 'knows': 6865,\n",
       " 'their': 11996,\n",
       " 'role': 10306,\n",
       " 'band': 1384,\n",
       " 'member': 7564,\n",
       " 'told': 12157,\n",
       " 'shit': 10792,\n",
       " 'gets': 5277,\n",
       " 'heavily': 5686,\n",
       " 'marketed': 7427,\n",
       " 'buy': 1981,\n",
       " 'buying': 1983,\n",
       " 'audience': 1280,\n",
       " 'small': 11031,\n",
       " 'brought': 1884,\n",
       " 'fashion': 4665,\n",
       " 'compare': 2626,\n",
       " 'through': 12081,\n",
       " 'talent': 11822,\n",
       " 'passion': 8704,\n",
       " 'hear': 5666,\n",
       " 'singer': 10925,\n",
       " 'singing': 10927,\n",
       " 'loss': 7241,\n",
       " 'sung': 11651,\n",
       " 'meaning': 7517,\n",
       " 'compared': 2628,\n",
       " 'real': 9764,\n",
       " 'singers': 10926,\n",
       " 'experience': 4496,\n",
       " 'continue': 2845,\n",
       " 'sell': 10646,\n",
       " 'decades': 3276,\n",
       " 'generations': 5250,\n",
       " 'sales': 10430,\n",
       " 'go': 5334,\n",
       " 'clubs': 2470,\n",
       " 'spotting': 11282,\n",
       " 'sounds': 11173,\n",
       " 'feels': 4723,\n",
       " 'running': 10379,\n",
       " 'bush': 1964,\n",
       " 'administrations': 648,\n",
       " 'suggests': 11629,\n",
       " 'raise': 9676,\n",
       " 'debt': 3266,\n",
       " 'limit': 7113,\n",
       " 'incidentally': 6163,\n",
       " 'available': 1310,\n",
       " 'us': 12626,\n",
       " 'edition': 4025,\n",
       " 'yahoo': 13275,\n",
       " 'international': 6441,\n",
       " 'versions': 12745,\n",
       " 'bad': 1361,\n",
       " 'traslate': 12265,\n",
       " 'word': 13173,\n",
       " 'rather': 9722,\n",
       " 'evaluating': 4382,\n",
       " 'expressions': 4542,\n",
       " 'sniff': 11068,\n",
       " 'server': 10697,\n",
       " 'finally': 4793,\n",
       " 'away': 1331,\n",
       " 'wise': 13124,\n",
       " 'offcourse': 8338,\n",
       " 'thing': 12042,\n",
       " 'humengous': 5943,\n",
       " 'amount': 888,\n",
       " 'war': 12896,\n",
       " 'moving': 7900,\n",
       " 'thankfully': 11980,\n",
       " '1996': 128,\n",
       " 'girlfriend': 5299,\n",
       " 'pacificnews': 8602,\n",
       " 'marko': 7433,\n",
       " '961001': 385,\n",
       " 'kevorkian': 6800,\n",
       " 'html': 5919,\n",
       " 'live': 7163,\n",
       " 'pleasant': 9012,\n",
       " 'apartment': 1002,\n",
       " 'berkeley': 1562,\n",
       " 'california': 2029,\n",
       " 'tons': 12171,\n",
       " 'alarming': 775,\n",
       " 'collection': 2530,\n",
       " 'cassettes': 2143,\n",
       " 'cd': 2183,\n",
       " 'never': 8101,\n",
       " 'heard': 5667,\n",
       " 'richard': 10228,\n",
       " 'hamming': 5570,\n",
       " 'says': 10476,\n",
       " 'amazingly': 860,\n",
       " 'graham': 5415,\n",
       " 'site': 10942,\n",
       " '25': 185,\n",
       " 'sites': 10943,\n",
       " 'shouldn': 10831,\n",
       " 'missed': 7731,\n",
       " 'designer': 3451,\n",
       " 'everybody': 4397,\n",
       " 'database': 3216,\n",
       " 'broken': 1878,\n",
       " 'happens': 5596,\n",
       " 'statutory': 11396,\n",
       " 'artificial': 1145,\n",
       " 'ceiling': 2188,\n",
       " 'imposed': 6117,\n",
       " 'congress': 2749,\n",
       " 'gives': 5307,\n",
       " 'allows': 824,\n",
       " 'treasury': 12272,\n",
       " 'restrict': 10143,\n",
       " 'spending': 11247,\n",
       " 'below': 1540,\n",
       " 'raises': 9678,\n",
       " 'needed': 8046,\n",
       " 'raised': 9677,\n",
       " '2002': 139,\n",
       " '2003': 140,\n",
       " '2004': 141,\n",
       " 'privateer': 9321,\n",
       " 'usdebt': 12634,\n",
       " 'antiwar': 981,\n",
       " 'articleid': 1139,\n",
       " '4015': 267,\n",
       " 'info': 6284,\n",
       " 'every': 4396,\n",
       " 'single': 10928,\n",
       " 'polyphasic': 9084,\n",
       " 'sleeping': 11002,\n",
       " 'thought': 12063,\n",
       " 'being': 1519,\n",
       " 'top': 12179,\n",
       " 'reddit': 9844,\n",
       " 'google': 5369,\n",
       " 'quality': 9603,\n",
       " 'engineer': 4217,\n",
       " 'details': 3476,\n",
       " 'banned': 1398,\n",
       " 'caught': 2164,\n",
       " 'stategies': 11378,\n",
       " 'disapproved': 3627,\n",
       " 'outspoken': 8553,\n",
       " 'learned': 7001,\n",
       " 'readers': 9756,\n",
       " 'chime': 2328,\n",
       " 'memorable': 7572,\n",
       " 'posts': 9151,\n",
       " 'webmaster': 12965,\n",
       " 'bookmark': 1744,\n",
       " 'future': 5153,\n",
       " 'reference': 9874,\n",
       " 'damn': 3191,\n",
       " 'fast': 4668,\n",
       " 'load': 7177,\n",
       " 'having': 5636,\n",
       " 'option': 8445,\n",
       " 'opening': 8407,\n",
       " 'image': 6058,\n",
       " 'window': 13102,\n",
       " 'wait': 12867,\n",
       " 'reload': 9983,\n",
       " 'closed': 2456,\n",
       " 'slow': 11021,\n",
       " 'little': 7161,\n",
       " 'patience': 8732,\n",
       " 'gimmicky': 5296,\n",
       " 'crap': 3018,\n",
       " 'waste': 12922,\n",
       " 'positive': 9130,\n",
       " 'living': 7168,\n",
       " 'police': 9054,\n",
       " 'state': 11375,\n",
       " 'study': 11532,\n",
       " 'couple': 2979,\n",
       " 'saying': 10475,\n",
       " 'average': 1315,\n",
       " 'citizen': 2386,\n",
       " '360': 253,\n",
       " 'die': 3557,\n",
       " 'auto': 1299,\n",
       " 'accident': 529,\n",
       " 'killed': 6822,\n",
       " 'terrorist': 11948,\n",
       " 'cars': 2128,\n",
       " 'quit': 9629,\n",
       " 'letting': 7054,\n",
       " 'fear': 4698,\n",
       " 'shortcut': 10818,\n",
       " 'ability': 481,\n",
       " 'murder': 7934,\n",
       " 'frequently': 5073,\n",
       " 'okay': 8369,\n",
       " 'easier': 3986,\n",
       " 'black': 1638,\n",
       " 'white': 13038,\n",
       " 'evil': 4407,\n",
       " 'makes': 7363,\n",
       " 'invest': 6511,\n",
       " 'effort': 4058,\n",
       " 'silly': 10893,\n",
       " 'too': 12173,\n",
       " 'rsheridan': 10349,\n",
       " 'however': 5914,\n",
       " 'our': 8517,\n",
       " 'elected': 4082,\n",
       " 'officals': 8348,\n",
       " 'record': 9823,\n",
       " 'deliberately': 3352,\n",
       " 'misleading': 7723,\n",
       " 'public': 9529,\n",
       " 'occasion': 8313,\n",
       " 'sure': 11696,\n",
       " 'meant': 7522,\n",
       " 'marked': 7422,\n",
       " 'wondering': 13162,\n",
       " 'noticed': 8224,\n",
       " 'glad': 5310,\n",
       " 'posted': 9141,\n",
       " 'wrong': 13234,\n",
       " 'conspiracy': 2791,\n",
       " 'nut': 8262,\n",
       " 'gullible': 5524,\n",
       " 'realize': 9772,\n",
       " 'robbed': 10286,\n",
       " 'ever': 4395,\n",
       " 'entry': 4276,\n",
       " 'website': 12969,\n",
       " 'ideas': 6006,\n",
       " 'spiff': 11255,\n",
       " 'insist': 6351,\n",
       " 'situation': 10947,\n",
       " 'worse': 13204,\n",
       " 'kids': 6818,\n",
       " 'flying': 4897,\n",
       " 'spagetti': 11196,\n",
       " 'monster': 7825,\n",
       " 'fruitful': 5102,\n",
       " 'multiply': 7926,\n",
       " 'bible': 1588,\n",
       " 'leave': 7008,\n",
       " 'brain': 1816,\n",
       " 'photos': 8906,\n",
       " 'sometimes': 11136,\n",
       " 'picture': 8930,\n",
       " 'unguarded': 12500,\n",
       " 'youth': 13317,\n",
       " 'individuals': 6234,\n",
       " 'successful': 11598,\n",
       " 'closely': 2457,\n",
       " 'eyes': 4569,\n",
       " 'smiles': 11051,\n",
       " 'open': 8405,\n",
       " 'pdf': 8759,\n",
       " 'follow': 4921,\n",
       " 'along': 834,\n",
       " 'mp3': 7903,\n",
       " 'yep': 13297,\n",
       " 'prostitution': 9477,\n",
       " 'victimless': 12765,\n",
       " 'crime': 3060,\n",
       " 'jesus': 6666,\n",
       " 'forgive': 4974,\n",
       " 'yes': 13298,\n",
       " 'touching': 12199,\n",
       " 'disabilities': 3613,\n",
       " 'somebody': 11127,\n",
       " 'problems': 9341,\n",
       " 'literal': 7153,\n",
       " 'btw': 1902,\n",
       " '_really_': 444,\n",
       " 'intimate': 6475,\n",
       " 'prostitute': 9475,\n",
       " 'opened': 8406,\n",
       " 'nice': 8131,\n",
       " 'pictures': 8931,\n",
       " 'title': 12137,\n",
       " 'expected': 4489,\n",
       " 'depth': 3427,\n",
       " 'pre': 9200,\n",
       " '80': 351,\n",
       " 'benevolent': 1557,\n",
       " 'british': 1872,\n",
       " 'oncologists': 8385,\n",
       " 'developed': 3503,\n",
       " 'revolutionary': 10204,\n",
       " 'prototype': 9490,\n",
       " 'dust': 3956,\n",
       " 'speck': 11230,\n",
       " 'sized': 10955,\n",
       " 'device': 3511,\n",
       " 'detect': 3478,\n",
       " 'cancer': 2059,\n",
       " 'seconds': 10596,\n",
       " 'aquaminds': 1068,\n",
       " 'breakthrough': 1840,\n",
       " 'collaboration': 2523,\n",
       " 'tool': 12175,\n",
       " 'allowing': 823,\n",
       " '36': 252,\n",
       " 'simultaneously': 10917,\n",
       " 'view': 12777,\n",
       " 'edit': 4022,\n",
       " 'text': 11965,\n",
       " 'document': 3776,\n",
       " 'accountants': 546,\n",
       " 'pharmacists': 8870,\n",
       " 'engineers': 4220,\n",
       " 'economy': 4014,\n",
       " 'fine': 4803,\n",
       " 'massage': 7456,\n",
       " 'therapists': 12024,\n",
       " 'store': 11457,\n",
       " 'clerks': 2431,\n",
       " 'delivery': 3358,\n",
       " 'folks': 4920,\n",
       " 'starting': 11370,\n",
       " 'worry': 13202,\n",
       " 'extra': 4553,\n",
       " 'jobs': 6684,\n",
       " 'food': 4930,\n",
       " 'table': 11793,\n",
       " 'misses': 7732,\n",
       " 'main': 7344,\n",
       " 'js': 6724,\n",
       " 'education': 4039,\n",
       " 'harder': 5608,\n",
       " 'concepts': 2693,\n",
       " 'whether': 13024,\n",
       " 'bright': 1861,\n",
       " 'control': 2873,\n",
       " 'over': 8556,\n",
       " 'environment': 4279,\n",
       " 'higgsboson': 5749,\n",
       " 'sharing': 10754,\n",
       " 'house': 5907,\n",
       " 'tried': 12296,\n",
       " 'doing': 3797,\n",
       " 'boss': 1772,\n",
       " 'co': 2483,\n",
       " 'won': 13158,\n",
       " 'unless': 12541,\n",
       " 'fire': 4817,\n",
       " 'still': 11428,\n",
       " 'interrupted': 6457,\n",
       " 'child': 2320,\n",
       " 'coming': 2578,\n",
       " 'share': 10746,\n",
       " 'idiot': 6020,\n",
       " 'noise': 8174,\n",
       " 'level': 7055,\n",
       " 'floor': 4880,\n",
       " '50': 295,\n",
       " 'loud': 7248,\n",
       " 'ringing': 10258,\n",
       " 'phones': 8895,\n",
       " 'listening': 7146,\n",
       " 'stereo': 11414,\n",
       " 'yet': 13303,\n",
       " 'manage': 7377,\n",
       " 'problem': 9339,\n",
       " '16': 77,\n",
       " 'hours': 5906,\n",
       " 'long': 7211,\n",
       " 'paid': 8617,\n",
       " 'waiting': 12870,\n",
       " 'laundry': 6969,\n",
       " 'sit': 10941,\n",
       " 'twiddling': 12375,\n",
       " 'thumbs': 12090,\n",
       " 'smoke': 11055,\n",
       " 'surrogate': 11713,\n",
       " 'married': 7438,\n",
       " 'forgot': 4976,\n",
       " 'adultery': 671,\n",
       " 'honor': 5861,\n",
       " 'mother': 7865,\n",
       " 'father': 4676,\n",
       " 'hey': 5738,\n",
       " 'modding': 7776,\n",
       " 'am': 855,\n",
       " 'defending': 3319,\n",
       " 'poster': 9142,\n",
       " 'irony': 6555,\n",
       " 'wwjd': 13245,\n",
       " 'yeah': 13286,\n",
       " 'favourite': 4690,\n",
       " 'brilliant': 1863,\n",
       " 'discussion': 3664,\n",
       " 'parallel': 8653,\n",
       " 'java': 6644,\n",
       " 'introduce': 6490,\n",
       " 'both': 1776,\n",
       " 'abcl': 478,\n",
       " 'standard': 11350,\n",
       " 'libraries': 7076,\n",
       " 'suggest': 11624,\n",
       " 'jython': 6766,\n",
       " 'complete': 2658,\n",
       " 'rubbish': 10354,\n",
       " 'feeling': 4721,\n",
       " 'provided': 9501,\n",
       " 'extremely': 4560,\n",
       " 'vague': 12666,\n",
       " 'refutations': 9900,\n",
       " 'concrete': 2707,\n",
       " 'specific': 11225,\n",
       " 'focuses': 4905,\n",
       " 'differences': 3563,\n",
       " 'between': 1576,\n",
       " 'painting': 8628,\n",
       " 'electronic': 4089,\n",
       " 'bits': 1630,\n",
       " 'putting': 9580,\n",
       " 'oil': 8365,\n",
       " 'canvas': 2076,\n",
       " 'stick': 11423,\n",
       " 'animal': 933,\n",
       " 'hair': 5556,\n",
       " 'concludes': 2704,\n",
       " 'utterly': 12658,\n",
       " 'each': 3970,\n",
       " 'clearly': 2429,\n",
       " 'programmer': 9400,\n",
       " 'matter': 7485,\n",
       " 'artists': 1149,\n",
       " 'writes': 13229,\n",
       " 'ones': 8387,\n",
       " ...}"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## our vocabulary\n",
    "bow.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can transform our text into our vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = bow.transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our data is a sparse matrix, which is a matrix with far more 0 values than not 0 values\n",
    "\n",
    "#### More about it herehttps://docs.scipy.org/doc/scipy/reference/sparse.html\n",
    "\n",
    "#### To make better sense of it, we can put it back into a dataframe. \n",
    "\n",
    "#### Caution: moving from sparse matrix to array format will make the calculations very computationally inefficient, and it is not recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = pd.DataFrame(training_data.toarray(),columns=bow.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>00000000000</th>\n",
       "      <th>0000000000000</th>\n",
       "      <th>00442070001201</th>\n",
       "      <th>00442071938940</th>\n",
       "      <th>0092647</th>\n",
       "      <th>01</th>\n",
       "      <th>01quackeryrelatedtopics</th>\n",
       "      <th>02</th>\n",
       "      <th>...</th>\n",
       "      <th>zogby</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zope</th>\n",
       "      <th>zsh</th>\n",
       "      <th>zur</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zyklon</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  13355 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  00000000000  0000000000000  00442070001201  00442071938940  \\\n",
       "0   0    0            0              0               0               0   \n",
       "1   0    0            0              0               0               0   \n",
       "2   0    0            0              0               0               0   \n",
       "3   0    0            0              0               0               0   \n",
       "4   0    0            0              0               0               0   \n",
       "\n",
       "   0092647  01  01quackeryrelatedtopics  02 ...   zogby  zombie  zone  zope  \\\n",
       "0        0   0                        0   0 ...       0       0     0     0   \n",
       "1        0   0                        0   0 ...       0       0     0     0   \n",
       "2        0   0                        0   0 ...       0       0     0     0   \n",
       "3        0   0                        0   0 ...       0       0     0     0   \n",
       "4        0   0                        0   0 ...       0       0     0     0   \n",
       "\n",
       "   zsh  zur  zurich  zyklon  zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  g  \n",
       "0    0    0       0       0                                            0    0  \n",
       "1    0    0       0       0                                            0    0  \n",
       "2    0    0       0       0                                            0    0  \n",
       "3    0    0       0       0                                            0    0  \n",
       "4    0    0       0       0                                            0    0  \n",
       "\n",
       "[5 rows x 13355 columns]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = CountVectorizer(ngram_range=(2,2))\n",
    "trigrams = CountVectorizer(ngram_range=(3,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(2, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00 and',\n",
       " '00 are',\n",
       " '00 html',\n",
       " '00 so',\n",
       " '00 to',\n",
       " '00 us',\n",
       " '000 00',\n",
       " '000 000',\n",
       " '000 acres',\n",
       " '000 excepting',\n",
       " '000 for',\n",
       " '000 found',\n",
       " '000 homes',\n",
       " '000 investment',\n",
       " '000 it',\n",
       " '000 million',\n",
       " '000 month',\n",
       " '000 nanometers',\n",
       " '000 or',\n",
       " '000 people',\n",
       " '000 rpm',\n",
       " '000 sgli',\n",
       " '000 since',\n",
       " '000 sq',\n",
       " '000 square',\n",
       " '000 to',\n",
       " '000 year',\n",
       " '00000000000 factorial',\n",
       " '0000000000000 factorial',\n",
       " '00442070001201 domain',\n",
       " '00442070001201 technical',\n",
       " '00442071938940 00442070001201',\n",
       " '0092647 5fencoding',\n",
       " '01 09',\n",
       " '01 11',\n",
       " '01 14',\n",
       " '01 16',\n",
       " '01 are',\n",
       " '01 colbert',\n",
       " '01 dec',\n",
       " '01 hindsights',\n",
       " '01 like',\n",
       " '01 similar',\n",
       " '01 think_in_weeks',\n",
       " '01 update_on_tech',\n",
       " '01quackeryrelatedtopics candling',\n",
       " '02 and',\n",
       " '03 06',\n",
       " '0385720270 qid',\n",
       " '04 13',\n",
       " '04 expires',\n",
       " '05 administrative',\n",
       " '05 not',\n",
       " '051010crat_atlarge which',\n",
       " '0521592712 still',\n",
       " '0525934189 ref',\n",
       " '0590025 6475319',\n",
       " '06 last',\n",
       " '06 state',\n",
       " '06 yet',\n",
       " '0612100 507846',\n",
       " '07 dangers',\n",
       " '07mm mechanical',\n",
       " '09 09_401',\n",
       " '09 30',\n",
       " '09 anthropomorphized',\n",
       " '09 diamond',\n",
       " '09 diamond_pr',\n",
       " '09 flame_someone_anonym',\n",
       " '09_401 html',\n",
       " '0s or',\n",
       " '10 000',\n",
       " '10 21',\n",
       " '10 50',\n",
       " '10 am',\n",
       " '10 barriers_are_yo',\n",
       " '10 commandments',\n",
       " '10 coworkers',\n",
       " '10 days',\n",
       " '10 default',\n",
       " '10 degrees',\n",
       " '10 file',\n",
       " '10 floor',\n",
       " '10 gallons',\n",
       " '10 have',\n",
       " '10 homes',\n",
       " '10 hours',\n",
       " '10 lines',\n",
       " '10 megawatt',\n",
       " '10 minutes',\n",
       " '10 more',\n",
       " '10 people',\n",
       " '10 rgggh',\n",
       " '10 runs',\n",
       " '10 seconds',\n",
       " '10 should',\n",
       " '10 some',\n",
       " '10 system',\n",
       " '10 tabs',\n",
       " '10 tiger',\n",
       " '10 times',\n",
       " '10 to',\n",
       " '10 trivial',\n",
       " '10 usd',\n",
       " '10 ve',\n",
       " '10 version',\n",
       " '10 wackiest',\n",
       " '10 will',\n",
       " '10 wouldn',\n",
       " '10 x10',\n",
       " '10 year',\n",
       " '10 years',\n",
       " '100 000',\n",
       " '100 150',\n",
       " '100 about',\n",
       " '100 accurate',\n",
       " '100 based',\n",
       " '100 bills',\n",
       " '100 black',\n",
       " '100 gt',\n",
       " '100 it',\n",
       " '100 million',\n",
       " '100 nanometers',\n",
       " '100 necessary',\n",
       " '100 of',\n",
       " '100 solution',\n",
       " '100 things',\n",
       " '100 think',\n",
       " '100 votes',\n",
       " '100 was',\n",
       " '100 what',\n",
       " '100 years',\n",
       " '100 zarutian',\n",
       " '1000 accounts',\n",
       " '1000 of',\n",
       " '10000103 amp',\n",
       " '100m behind',\n",
       " '100mhz as',\n",
       " '102 0590025',\n",
       " '102 104',\n",
       " '102 5529811',\n",
       " '10260 that',\n",
       " '103 7750701',\n",
       " '103480 amp',\n",
       " '104 search',\n",
       " '105 inch',\n",
       " '10837 lot',\n",
       " '10k year',\n",
       " '10m overhead',\n",
       " '10th century',\n",
       " '10th grade',\n",
       " '10th station',\n",
       " '10th under',\n",
       " '10yo girls',\n",
       " '11 09',\n",
       " '11 15',\n",
       " '11 21',\n",
       " '11 _absolutely_',\n",
       " '11 afaik',\n",
       " '11 after',\n",
       " '11 attacks',\n",
       " '11 because',\n",
       " '11 bill',\n",
       " '11 bolero_pr',\n",
       " '11 hours',\n",
       " '11 html',\n",
       " '11 keep',\n",
       " '11 minutes',\n",
       " '11 points',\n",
       " '11 story',\n",
       " '11 the_zen_estheti',\n",
       " '11 theories',\n",
       " '11 this',\n",
       " '11 tivo',\n",
       " '11 use',\n",
       " '11 was',\n",
       " '11 years',\n",
       " '113 points',\n",
       " '1136132459 sr',\n",
       " '11519 it',\n",
       " '11th got',\n",
       " '12 11',\n",
       " '12 22',\n",
       " '12 23',\n",
       " '12 50',\n",
       " '12 am',\n",
       " '12 cdn',\n",
       " '12 credit',\n",
       " '12 feet',\n",
       " '12 grade',\n",
       " '12 in',\n",
       " '12 intel',\n",
       " '12 market_utopians',\n",
       " '12 months',\n",
       " '12 new_years_perso',\n",
       " '12 or',\n",
       " '12 points',\n",
       " '12 powerbook',\n",
       " '12 steps',\n",
       " '12 to',\n",
       " '12 us',\n",
       " '12 year',\n",
       " '12 years',\n",
       " '124 but',\n",
       " '1286 61091',\n",
       " '12oz cup',\n",
       " '12processo_eng php',\n",
       " '13 11',\n",
       " '13 days',\n",
       " '13 extra',\n",
       " '13 google',\n",
       " '13 my',\n",
       " '13 years',\n",
       " '130 credit',\n",
       " '130 mod',\n",
       " '130kw to',\n",
       " '130kw when',\n",
       " '1318 emacs',\n",
       " '14 000',\n",
       " '14 10',\n",
       " '14 207',\n",
       " '14 32',\n",
       " '14 amp',\n",
       " '14 do',\n",
       " '14 flickr',\n",
       " '14 pkr',\n",
       " '14 with',\n",
       " '14107 after',\n",
       " '148 to',\n",
       " '14e2xpi usd',\n",
       " '14th the',\n",
       " '15 1971',\n",
       " '15 67248daf452325d0',\n",
       " '15 amp',\n",
       " '15 and',\n",
       " '15 are',\n",
       " '15 comments',\n",
       " '15 credit',\n",
       " '15 hours',\n",
       " '15 isn',\n",
       " '15 minute',\n",
       " '15 minutes',\n",
       " '15 years',\n",
       " '150 000',\n",
       " '150 bargain',\n",
       " '150 gt',\n",
       " '150 mo',\n",
       " '150 second',\n",
       " '150 years',\n",
       " '150k range',\n",
       " '150k salaries',\n",
       " '1512 pdf',\n",
       " '15541 11',\n",
       " '1581345615 he',\n",
       " '15k month',\n",
       " '15th click',\n",
       " '15th of',\n",
       " '16 72',\n",
       " '16 al',\n",
       " '16 and',\n",
       " '16 credits',\n",
       " '16 grade',\n",
       " '16 hour',\n",
       " '16 hours',\n",
       " '16 minutes',\n",
       " '16 on',\n",
       " '16 or',\n",
       " '16 underage',\n",
       " '1600 when',\n",
       " '1605 ggccebro',\n",
       " '1628 on',\n",
       " '163 and',\n",
       " '165m of',\n",
       " '16s within',\n",
       " '17 macbook',\n",
       " '17 years',\n",
       " '177 cc',\n",
       " '18 before',\n",
       " '18 year',\n",
       " '1880 with',\n",
       " '189cm and',\n",
       " '19 days',\n",
       " '19 hours',\n",
       " '19 times',\n",
       " '191 56',\n",
       " '191 57',\n",
       " '191 58',\n",
       " '191 grade',\n",
       " '191 points',\n",
       " '1918944 00',\n",
       " '192 pp',\n",
       " '1921 and',\n",
       " '1921 to',\n",
       " '1929 amp',\n",
       " '1929 inflation',\n",
       " '1936 has',\n",
       " '1942 is',\n",
       " '19459 we',\n",
       " '1946 every',\n",
       " '1948 it',\n",
       " '1948 was',\n",
       " '1949 my',\n",
       " '1966 ford',\n",
       " '1970 1971',\n",
       " '1971 foreigners',\n",
       " '1971 while',\n",
       " '1974 boundary',\n",
       " '1978 in',\n",
       " '1980s what',\n",
       " '1981 192',\n",
       " '1981 has',\n",
       " '1981 they',\n",
       " '1982 the',\n",
       " '1984 reports',\n",
       " '1986 and',\n",
       " '1987 long',\n",
       " '1989 http',\n",
       " '1990 miata',\n",
       " '1990 or',\n",
       " '1991 dr',\n",
       " '1993 john',\n",
       " '1993 what',\n",
       " '1994 home',\n",
       " '1994 or',\n",
       " '1995 its',\n",
       " '1996 he',\n",
       " '1997 http',\n",
       " '1997 to',\n",
       " '1998 it',\n",
       " '1999 2000',\n",
       " '1999 definitely',\n",
       " '1999 he',\n",
       " '19th century',\n",
       " '1st amendment',\n",
       " '1st century',\n",
       " '1y amp',\n",
       " '20 20',\n",
       " '20 25',\n",
       " '20 and',\n",
       " '20 can',\n",
       " '20 days',\n",
       " '20 episode',\n",
       " '20 floors',\n",
       " '20 here',\n",
       " '20 in',\n",
       " '20 million',\n",
       " '20 minutes',\n",
       " '20 of',\n",
       " '20 times',\n",
       " '20 to',\n",
       " '20 us',\n",
       " '20 year',\n",
       " '20 years',\n",
       " '200 factorial',\n",
       " '200 square',\n",
       " '200 times',\n",
       " '200 tip',\n",
       " '200 with',\n",
       " '200 years',\n",
       " '200 g',\n",
       " '2000 at',\n",
       " '2000 xp',\n",
       " '2000 years',\n",
       " '2001 cholera',\n",
       " '2001 http',\n",
       " '2001 it',\n",
       " '2001 right',\n",
       " '2001 still',\n",
       " '2001 version',\n",
       " '2001 we',\n",
       " '2002 2003',\n",
       " '2002 50',\n",
       " '2002 can',\n",
       " '2002 cdn',\n",
       " '2002 that',\n",
       " '2002 there',\n",
       " '2003 and',\n",
       " '2003 nt',\n",
       " '2004 09',\n",
       " '2004 according',\n",
       " '2004 and',\n",
       " '2004 for',\n",
       " '2004 see',\n",
       " '2004 version',\n",
       " '20041015134125 http',\n",
       " '2005 03',\n",
       " '2005 04',\n",
       " '2005 07',\n",
       " '2005 09',\n",
       " '2005 10',\n",
       " '2005 11',\n",
       " '2005 12',\n",
       " '2005 after',\n",
       " '2005 and',\n",
       " '2005 date',\n",
       " '2005 he',\n",
       " '2005 i686',\n",
       " '2005 melosh',\n",
       " '2005 see',\n",
       " '2005 that',\n",
       " '2005 the',\n",
       " '2005 which',\n",
       " '2005 x86_64',\n",
       " '20051212p2g00m0bu028000c html',\n",
       " '2005_03_14 article1',\n",
       " '2006 01',\n",
       " '2006 but',\n",
       " '2006 in',\n",
       " '2006 not',\n",
       " '2006 or',\n",
       " '2006 probable',\n",
       " '2006 they',\n",
       " '20060112 ap_on_go_pr_wh',\n",
       " '2008 we',\n",
       " '200k files',\n",
       " '2010 soccer',\n",
       " '2020 allowed',\n",
       " '20200 and',\n",
       " '2035 meaning',\n",
       " '2050 how',\n",
       " '207 104',\n",
       " '2074 posted',\n",
       " '20economy israel',\n",
       " '20reddit com',\n",
       " '20th century',\n",
       " '20the 20economy',\n",
       " '21 000',\n",
       " '21 10',\n",
       " '21 32',\n",
       " '21 html',\n",
       " '21 years',\n",
       " '22 24',\n",
       " '22 amp',\n",
       " '22 and',\n",
       " '22 elsmp',\n",
       " '22 finalists',\n",
       " '22 gt',\n",
       " '22 thing',\n",
       " '22no happy',\n",
       " '22socialism 22',\n",
       " '23 2005',\n",
       " '23 adapted',\n",
       " '23 pound',\n",
       " '23 wikipedia_gift_spurned',\n",
       " '2357 took',\n",
       " '237 cc',\n",
       " '24 hour',\n",
       " '24 may',\n",
       " '2411 this',\n",
       " '24560 you',\n",
       " '248 72',\n",
       " '249 72',\n",
       " '24carat co',\n",
       " '24dot1 com',\n",
       " '25 based',\n",
       " '25 cc',\n",
       " '25 days',\n",
       " '25 jan',\n",
       " '25 miles',\n",
       " '25 web',\n",
       " '25 year',\n",
       " '25 years',\n",
       " '250 000',\n",
       " '250 years',\n",
       " '250th sec',\n",
       " '25937 together',\n",
       " '25961 43',\n",
       " '25cc of',\n",
       " '26 years',\n",
       " '260 houses',\n",
       " '260 zeros',\n",
       " '26495 c5437',\n",
       " '26599 is',\n",
       " '26615 love',\n",
       " '26959 points',\n",
       " '27 before',\n",
       " '27 pretty',\n",
       " '27147 edited',\n",
       " '2760 while',\n",
       " '2858 you',\n",
       " '29 14',\n",
       " '29 to',\n",
       " '2b 2b',\n",
       " '2b 2cc',\n",
       " '2b 2cperl',\n",
       " '2c and',\n",
       " '2c machete',\n",
       " '2c mushroom',\n",
       " '2c nuclear',\n",
       " '2c snake',\n",
       " '2cagent smith',\n",
       " '2castronaut 2csperm',\n",
       " '2cc 2b',\n",
       " '2cc 2chtml',\n",
       " '2cerlang 2c',\n",
       " '2cerlang and',\n",
       " '2cfortran 2csmalltalk',\n",
       " '2chaskell 2cpython',\n",
       " '2chaskell developer',\n",
       " '2chaskell programmer',\n",
       " '2cjava 2cc',\n",
       " '2cjava 2cphp',\n",
       " '2clisp 2cfortran',\n",
       " '2clisp 2csmalltalk',\n",
       " '2cphp 2cjava',\n",
       " '2cphp is',\n",
       " '2cphp just',\n",
       " '2cpirate 2cagent',\n",
       " '2cpirate in',\n",
       " '2cpython 2cc',\n",
       " '2cpython 2cerlang',\n",
       " '2cpython 2cjava',\n",
       " '2cpython 2cperl',\n",
       " '2cpython 2cphp',\n",
       " '2cpython 2csex',\n",
       " '2cpython 2curine',\n",
       " '2cpython developer',\n",
       " '2cpython programmer',\n",
       " '2crocket scientist',\n",
       " '2cruby 2cerlang',\n",
       " '2cruby 2chaskell',\n",
       " '2cruby 2cpython',\n",
       " '2cruby developer',\n",
       " '2cruby for',\n",
       " '2cruby programmer',\n",
       " '2csex then',\n",
       " '2csmalltalk 2cruby',\n",
       " '2csmalltalk developer',\n",
       " '2csmalltalk programmer',\n",
       " '2csperm donor',\n",
       " '2cunderwater basket',\n",
       " '2d graphics',\n",
       " '2kw and',\n",
       " '2liter cylinder',\n",
       " '2m 45s',\n",
       " '2mm accuracy',\n",
       " '2nd hand',\n",
       " '2nd paragraph',\n",
       " '2nd rate',\n",
       " '2sf sicko',\n",
       " '2y amp',\n",
       " '30 000',\n",
       " '30 after',\n",
       " '30 days',\n",
       " '30 degrees',\n",
       " '30 fewer',\n",
       " '30 fun',\n",
       " '30 gpa',\n",
       " '30 have',\n",
       " '30 hour',\n",
       " '30 minutes',\n",
       " '30 of',\n",
       " '30 second',\n",
       " '30 seconds',\n",
       " '30 times',\n",
       " '30 to',\n",
       " '30 years',\n",
       " '300 amp',\n",
       " '3000 month',\n",
       " '3000 per',\n",
       " '30414093201 512000000000000',\n",
       " '32 36',\n",
       " '32 49',\n",
       " '32 node',\n",
       " '32 which',\n",
       " '33 000',\n",
       " '33 amp',\n",
       " '33 eclipse',\n",
       " '331 best',\n",
       " '3339 did',\n",
       " '34 knew',\n",
       " '342 85',\n",
       " '35 bbl',\n",
       " '35 points',\n",
       " '35 years',\n",
       " '350 million',\n",
       " '355 cc',\n",
       " '36 000',\n",
       " '36 bst',\n",
       " '36 different',\n",
       " '36 hours',\n",
       " '36 people',\n",
       " '360 times',\n",
       " '36k and',\n",
       " '37 signals',\n",
       " '37 week',\n",
       " '37103 reddit',\n",
       " '37signals message',\n",
       " '38 but',\n",
       " '3db lower',\n",
       " '3dblended amp',\n",
       " '3g networks',\n",
       " '3rd and',\n",
       " '3rd century',\n",
       " '3rd corner',\n",
       " '3rd party',\n",
       " '3rd time',\n",
       " '3rd world',\n",
       " '3x4 12',\n",
       " '40 000',\n",
       " '40 but',\n",
       " '40 gates',\n",
       " '40 in',\n",
       " '40 is',\n",
       " '40 liter',\n",
       " '40 of',\n",
       " '40 productivity',\n",
       " '40 salary',\n",
       " '40 the',\n",
       " '40 week',\n",
       " '40 years',\n",
       " '400 000',\n",
       " '400 and',\n",
       " '400 people',\n",
       " '400 per',\n",
       " '400 years',\n",
       " '4000th love',\n",
       " '4015 for',\n",
       " '403 is',\n",
       " '404 errors',\n",
       " '404 image',\n",
       " '41 and',\n",
       " '41 this',\n",
       " '41 to',\n",
       " '41505 41505',\n",
       " '41505 html',\n",
       " '42 inch',\n",
       " '42 yr',\n",
       " '4202741 stm',\n",
       " '43 points',\n",
       " '44 bc',\n",
       " '44 joe',\n",
       " '44 usd',\n",
       " '444 44',\n",
       " '44cc then',\n",
       " '45 degree',\n",
       " '45 liberal',\n",
       " '453 second',\n",
       " '455 grade',\n",
       " '455 or',\n",
       " '458 46',\n",
       " '4598714 stm',\n",
       " '46 cumulative',\n",
       " '47 of',\n",
       " '48 hours',\n",
       " '49 utc',\n",
       " '4d257aac09496b6d 67248daf452325d0',\n",
       " '4q cc',\n",
       " '4th centuries',\n",
       " '4th floor',\n",
       " '4ths vote',\n",
       " '4x laying',\n",
       " '50 000',\n",
       " '50 45',\n",
       " '50 50',\n",
       " '50 amazon',\n",
       " '50 bbl',\n",
       " '50 cdn',\n",
       " '50 chance',\n",
       " '50 each',\n",
       " '50 environmental',\n",
       " '50 greatest',\n",
       " '50 group',\n",
       " '50 gt',\n",
       " '50 in',\n",
       " '50 java',\n",
       " '50 likely',\n",
       " '50 loud',\n",
       " '50 ms',\n",
       " '50 of',\n",
       " '50 people',\n",
       " '50 simple',\n",
       " '50 this',\n",
       " '50 trivial',\n",
       " '50 us',\n",
       " '50 who',\n",
       " '50 year',\n",
       " '50 years',\n",
       " '50 you',\n",
       " '500 billion',\n",
       " '500 comments',\n",
       " '500 error',\n",
       " '500 guns',\n",
       " '500 million',\n",
       " '500 to',\n",
       " '500 years',\n",
       " '507846 amp',\n",
       " '51 52',\n",
       " '512000000000000 factorial',\n",
       " '52 have',\n",
       " '52 if',\n",
       " '52 recursions',\n",
       " '53 000',\n",
       " '53 users',\n",
       " '5385434 the',\n",
       " '54238 london',\n",
       " '5529811 0612100',\n",
       " '56 16',\n",
       " '56 41',\n",
       " '56 credit',\n",
       " '56 speak',\n",
       " '5648 html',\n",
       " '567 8688',\n",
       " '57 72',\n",
       " '57 grade',\n",
       " '57 varieties',\n",
       " '57 years',\n",
       " '5713383956 0000000000000',\n",
       " '58 000',\n",
       " '58 72',\n",
       " '58 grade',\n",
       " '58 of',\n",
       " '59 would',\n",
       " '591 cc',\n",
       " '598einst htm',\n",
       " '5b 5d',\n",
       " '5d dollar',\n",
       " '5d gdpcp',\n",
       " '5d gdpdeflation',\n",
       " '5d nominalgdp',\n",
       " '5d unskilled',\n",
       " '5fc3l4jqjvuj fie',\n",
       " '5fencoding utf8',\n",
       " '5kw when',\n",
       " '5y foster',\n",
       " '60 000',\n",
       " '60 50',\n",
       " '60 or',\n",
       " '60 the',\n",
       " '60 times',\n",
       " '60 watt',\n",
       " '600 out',\n",
       " '6000 liters',\n",
       " '6000 per',\n",
       " '60k 150k',\n",
       " '60k to',\n",
       " '60k year',\n",
       " '61091 00',\n",
       " '63 points',\n",
       " '630 html',\n",
       " '637 14',\n",
       " '64 ad',\n",
       " '6475319 url',\n",
       " '66 102',\n",
       " '66 books',\n",
       " '661 of',\n",
       " '67248daf452325d0 lnk',\n",
       " '68k assembler',\n",
       " '68k based',\n",
       " '68k machine',\n",
       " '69946 html',\n",
       " '6v6gt com',\n",
       " '70 degrees',\n",
       " '70 early',\n",
       " '70 efficient',\n",
       " '70 lbs',\n",
       " '70 of',\n",
       " '70 people',\n",
       " '7000 known',\n",
       " '70s and',\n",
       " '72 14',\n",
       " '72 248',\n",
       " '72 249',\n",
       " '72 444',\n",
       " '72 458',\n",
       " '72 hours',\n",
       " '737 when',\n",
       " '747 landing',\n",
       " '747 thundering',\n",
       " '75 but',\n",
       " '75 in',\n",
       " '75 needed',\n",
       " '75 of',\n",
       " '75 posts',\n",
       " '757 at',\n",
       " '76 amazon',\n",
       " '76ers recent',\n",
       " '7750701 0092647',\n",
       " '78 so',\n",
       " '7th day',\n",
       " '80 86',\n",
       " '80 icl',\n",
       " '80 in',\n",
       " '80 marks',\n",
       " '80 microsoft',\n",
       " '80 million',\n",
       " '80 of',\n",
       " '8000 main',\n",
       " '8000 words',\n",
       " '8088 ll',\n",
       " '8090 video',\n",
       " '80mb for',\n",
       " '80s since',\n",
       " '81 now',\n",
       " '8123 1918944',\n",
       " '85 billion',\n",
       " '85 us',\n",
       " '85 world',\n",
       " '86 http',\n",
       " '8688 you',\n",
       " '87 is',\n",
       " '87 of',\n",
       " '88 000',\n",
       " '88 an',\n",
       " '8818454 slashdot',\n",
       " '888 567',\n",
       " '89 95',\n",
       " '89q3 heardcomet',\n",
       " '8hrs early',\n",
       " '8note57 html',\n",
       " '8oz appears',\n",
       " '8oz size',\n",
       " '8th floor',\n",
       " '90 degree',\n",
       " '90 everyone',\n",
       " '90 from',\n",
       " '90 of',\n",
       " '90 per',\n",
       " '900 years',\n",
       " '900584 let',\n",
       " '90s that',\n",
       " '911research wtc7',\n",
       " '93 neither',\n",
       " '933262154439 00000000000',\n",
       " '93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000 etc',\n",
       " '940 and',\n",
       " '948 of',\n",
       " '95 death',\n",
       " '95 month',\n",
       " '95 of',\n",
       " '95 profit',\n",
       " '96 of',\n",
       " '96 to',\n",
       " '961001 kevorkian',\n",
       " '97 dead',\n",
       " '97 google',\n",
       " '97 the_earth_seen_from_apollo_17',\n",
       " '97mojo_400 profile14',\n",
       " '98 has',\n",
       " '99 huh',\n",
       " '99 of',\n",
       " '996 btus',\n",
       " '9kwh liter',\n",
       " '9za gb',\n",
       " '______ using',\n",
       " '_a connoisseur',\n",
       " '_a lot_',\n",
       " '_absolutely_ did',\n",
       " '_again you',\n",
       " '_all this',\n",
       " '_atheist_ there',\n",
       " '_belief_ is',\n",
       " '_believe_ after',\n",
       " '_believe_ it',\n",
       " '_best_ clarification',\n",
       " '_billion_ dollars',\n",
       " '_bit_ differently',\n",
       " '_capitol hill',\n",
       " '_chatterjee added',\n",
       " '_christians_ _believe_',\n",
       " '_could_ be',\n",
       " '_decrease_ the',\n",
       " '_dis_proven is',\n",
       " '_discussed_ early',\n",
       " '_explain_ the',\n",
       " '_for_ hugh',\n",
       " '_fox conceded',\n",
       " '_global_ production',\n",
       " '_highly_ contigent',\n",
       " '_how does',\n",
       " '_how low',\n",
       " '_i promise',\n",
       " '_in the',\n",
       " '_incredibly_ hard',\n",
       " '_independent_ studies',\n",
       " '_iraq_ it',\n",
       " '_is_ instantaneous',\n",
       " '_is_ leap',\n",
       " '_know_ about',\n",
       " '_list peer',\n",
       " '_long long_',\n",
       " '_maarten characteristics_and_tourist_information',\n",
       " '_many of',\n",
       " '_mathematica_ ve',\n",
       " '_more_ sense',\n",
       " '_most of',\n",
       " '_mother teresa',\n",
       " '_next_ life',\n",
       " '_nobody except',\n",
       " '_not_ net',\n",
       " '_not_ to',\n",
       " '_now_ while',\n",
       " '_okay so',\n",
       " '_one question',\n",
       " '_other people_',\n",
       " '_other_ vice',\n",
       " '_overwhelmingly_ rich',\n",
       " '_people who',\n",
       " '_persistent_ early',\n",
       " '_really just',\n",
       " '_really smart_',\n",
       " '_really_ believe',\n",
       " '_really_ don',\n",
       " '_religious_ one',\n",
       " '_same_ set',\n",
       " '_scientifically_ we',\n",
       " '_should_ work',\n",
       " '_so my',\n",
       " '_the great',\n",
       " '_the next',\n",
       " '_their_ lives',\n",
       " '_this_ is',\n",
       " '_totally_ justifies',\n",
       " '_verify_ that',\n",
       " '_why_ the',\n",
       " '_wille zur',\n",
       " '_with growing',\n",
       " '_you are',\n",
       " '_you_ state',\n",
       " 'a3 mobile',\n",
       " 'a3 msi',\n",
       " 'a3 solution',\n",
       " 'a4 wastage',\n",
       " 'a_bayes and',\n",
       " 'aaaaaaaaaaaaaarrrrrgh most',\n",
       " 'aac could',\n",
       " 'aacs advanced',\n",
       " 'aaron if',\n",
       " 'aaron what',\n",
       " 'aaronkoblin com',\n",
       " 'aaronsw com',\n",
       " 'aaronsw would',\n",
       " 'aasted org',\n",
       " 'abandon myspace',\n",
       " 'abandon the',\n",
       " 'abandon them',\n",
       " 'abandoned million',\n",
       " 'abandoned um',\n",
       " 'abandoned with',\n",
       " 'abandoning our',\n",
       " 'abbie hoffman',\n",
       " 'abc and',\n",
       " 'abcl and',\n",
       " 'aberrations need',\n",
       " 'abilities besides',\n",
       " 'abilities in',\n",
       " 'abilities to',\n",
       " 'ability http',\n",
       " 'ability monopolies',\n",
       " 'ability not',\n",
       " 'ability to',\n",
       " 'abiogenisis and',\n",
       " 'abiogenisis is',\n",
       " 'abiogenisis like',\n",
       " 'abismal state',\n",
       " 'able and',\n",
       " 'able to',\n",
       " 'ablity to',\n",
       " 'abnoxious page',\n",
       " 'abomination like',\n",
       " 'abortion presidential',\n",
       " 'about 10',\n",
       " 'about 100',\n",
       " 'about 100m',\n",
       " 'about 10m',\n",
       " 'about 11',\n",
       " 'about 130',\n",
       " 'about 15',\n",
       " 'about 150',\n",
       " 'about 20',\n",
       " 'about 200',\n",
       " 'about 2050',\n",
       " 'about 2kw',\n",
       " 'about 40',\n",
       " 'about 400',\n",
       " 'about 50',\n",
       " 'about 500',\n",
       " 'about 5kw',\n",
       " 'about 70',\n",
       " 'about 80',\n",
       " 'about 97',\n",
       " 'about 99',\n",
       " 'about _their_',\n",
       " 'about about',\n",
       " 'about above',\n",
       " 'about ac40',\n",
       " 'about actual',\n",
       " 'about alan',\n",
       " 'about algorithms',\n",
       " 'about all',\n",
       " 'about an',\n",
       " 'about and',\n",
       " 'about animal',\n",
       " 'about any',\n",
       " 'about anything',\n",
       " 'about apparently',\n",
       " 'about appears',\n",
       " 'about aren',\n",
       " 'about artificial',\n",
       " 'about as',\n",
       " ...]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00 and had',\n",
       " '00 are owned',\n",
       " '00 html tw',\n",
       " '00 so saying',\n",
       " '00 to the',\n",
       " '00 us now',\n",
       " '000 00 are',\n",
       " '000 000 00',\n",
       " '000 000 found',\n",
       " '000 000 million',\n",
       " '000 000 people',\n",
       " '000 000 since',\n",
       " '000 acres and',\n",
       " '000 excepting very',\n",
       " '000 for work',\n",
       " '000 found that',\n",
       " '000 homes 21',\n",
       " '000 homes if',\n",
       " '000 investment for',\n",
       " '000 it was',\n",
       " '000 million short',\n",
       " '000 nanometers thick',\n",
       " '000 or so',\n",
       " '000 people are',\n",
       " '000 rpm hard',\n",
       " '000 sgli life',\n",
       " '000 since the',\n",
       " '000 sq ft',\n",
       " '000 square feet',\n",
       " '000 to be',\n",
       " '000 year but',\n",
       " '000 year mckinsey',\n",
       " '000 year old',\n",
       " '00000000000 factorial acc',\n",
       " '0000000000000 factorial acc',\n",
       " '00442070001201 domain servers',\n",
       " '00442070001201 technical contact',\n",
       " '00442071938940 00442070001201 domain',\n",
       " '00442071938940 00442070001201 technical',\n",
       " '0092647 5fencoding utf8',\n",
       " '01 09 flame_someone_anonym',\n",
       " '01 11 html',\n",
       " '01 14 do',\n",
       " '01 16 al',\n",
       " '01 are there',\n",
       " '01 colbert reports',\n",
       " '01 dec 05',\n",
       " '01 hindsights html',\n",
       " '01 like madd',\n",
       " '01 similar posts',\n",
       " '01 think_in_weeks html',\n",
       " '01 update_on_tech html',\n",
       " '01quackeryrelatedtopics candling html',\n",
       " '02 and bought',\n",
       " '03 06 state',\n",
       " '0385720270 qid 1136132459',\n",
       " '04 13 my',\n",
       " '04 expires on',\n",
       " '05 administrative contact',\n",
       " '05 not imho',\n",
       " '051010crat_atlarge which was',\n",
       " '0521592712 still less',\n",
       " '0525934189 ref cm_cr_dp_pt',\n",
       " '0590025 6475319 url',\n",
       " '06 last updated',\n",
       " '06 state schools_still_rank_ne',\n",
       " '06 yet everything',\n",
       " '0612100 507846 amp',\n",
       " '07 dangers of',\n",
       " '07mm mechanical pencils',\n",
       " '09 09_401 html',\n",
       " '09 30 fun',\n",
       " '09 anthropomorphized so',\n",
       " '09 diamond html',\n",
       " '09 diamond_pr html',\n",
       " '09 flame_someone_anonym html',\n",
       " '09_401 html on',\n",
       " '0s or maybe',\n",
       " '10 000 rpm',\n",
       " '10 000 to',\n",
       " '10 21 html',\n",
       " '10 50 of',\n",
       " '10 am so',\n",
       " '10 barriers_are_yo html',\n",
       " '10 commandments put',\n",
       " '10 coworkers returns',\n",
       " '10 days of',\n",
       " '10 default tue',\n",
       " '10 degrees centigrade',\n",
       " '10 file sharing',\n",
       " '10 floor elevator',\n",
       " '10 gallons of',\n",
       " '10 have no',\n",
       " '10 homes in',\n",
       " '10 hours the',\n",
       " '10 lines of',\n",
       " '10 megawatt power',\n",
       " '10 minutes because',\n",
       " '10 minutes for',\n",
       " '10 minutes of',\n",
       " '10 minutes to',\n",
       " '10 more would',\n",
       " '10 people to',\n",
       " '10 people would',\n",
       " '10 rgggh this',\n",
       " '10 runs to',\n",
       " '10 seconds at',\n",
       " '10 should higher',\n",
       " '10 should the',\n",
       " '10 some 200',\n",
       " '10 system what',\n",
       " '10 tabs open',\n",
       " '10 tiger only',\n",
       " '10 times what',\n",
       " '10 to 15',\n",
       " '10 trivial example',\n",
       " '10 usd now',\n",
       " '10 ve got',\n",
       " '10 version 10',\n",
       " '10 wackiest conspiracy',\n",
       " '10 will not',\n",
       " '10 wouldn have',\n",
       " '10 x10 shaft',\n",
       " '10 year old',\n",
       " '10 years 1986',\n",
       " '10 years old',\n",
       " '100 000 nanometers',\n",
       " '100 000 year',\n",
       " '100 150 second',\n",
       " '100 about groupthink',\n",
       " '100 accurate like',\n",
       " '100 based on',\n",
       " '100 bills from',\n",
       " '100 black ha',\n",
       " '100 gt 93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000',\n",
       " '100 gt acc',\n",
       " '100 it runs',\n",
       " '100 million dollars',\n",
       " '100 million in',\n",
       " '100 million it',\n",
       " '100 million yet',\n",
       " '100 nanometers in',\n",
       " '100 necessary to',\n",
       " '100 of the',\n",
       " '100 of your',\n",
       " '100 solution for',\n",
       " '100 things started',\n",
       " '100 think something',\n",
       " '100 votes the',\n",
       " '100 was actually',\n",
       " '100 what about',\n",
       " '100 years ago',\n",
       " '100 years emissions',\n",
       " '100 years from',\n",
       " '100 years methane',\n",
       " '100 years of',\n",
       " '100 years old',\n",
       " '100 zarutian it',\n",
       " '1000 accounts be',\n",
       " '1000 of years',\n",
       " '10000103 amp sid',\n",
       " '100m behind 737',\n",
       " '100mhz as wireless',\n",
       " '102 0590025 6475319',\n",
       " '102 104 search',\n",
       " '102 5529811 0612100',\n",
       " '10260 that is',\n",
       " '103 7750701 0092647',\n",
       " '103480 amp cid',\n",
       " '104 search cache',\n",
       " '105 inch screen',\n",
       " '10837 lot of',\n",
       " '10k year and',\n",
       " '10m overhead even',\n",
       " '10th century ad',\n",
       " '10th grade ll',\n",
       " '10th station to',\n",
       " '10th under the',\n",
       " '10yo girls so',\n",
       " '11 09 diamond',\n",
       " '11 09 diamond_pr',\n",
       " '11 15 are',\n",
       " '11 21 10',\n",
       " '11 _absolutely_ did',\n",
       " '11 afaik it',\n",
       " '11 after building',\n",
       " '11 attacks were',\n",
       " '11 because you',\n",
       " '11 bill moyers',\n",
       " '11 bolero_pr html',\n",
       " '11 hours ago',\n",
       " '11 html for',\n",
       " '11 keep in',\n",
       " '11 minutes to',\n",
       " '11 points month',\n",
       " '11 story are',\n",
       " '11 the_zen_estheti html',\n",
       " '11 theories of',\n",
       " '11 this guy',\n",
       " '11 tivo buyout',\n",
       " '11 use valgrind',\n",
       " '11 was orchestrated',\n",
       " '11 years my',\n",
       " '113 points http',\n",
       " '1136132459 sr ref',\n",
       " '11519 it racked',\n",
       " '11th got me',\n",
       " '12 11 keep',\n",
       " '12 22 thing',\n",
       " '12 23 wikipedia_gift_spurned',\n",
       " '12 50 this',\n",
       " '12 am on',\n",
       " '12 cdn 00',\n",
       " '12 credit hour',\n",
       " '12 grade points',\n",
       " '12 in physics',\n",
       " '12 intel is',\n",
       " '12 market_utopians html',\n",
       " '12 new_years_perso html',\n",
       " '12 or points',\n",
       " '12 points for',\n",
       " '12 points the',\n",
       " '12 powerbook which',\n",
       " '12 steps to',\n",
       " '12 to 12',\n",
       " '12 us do',\n",
       " '12 year old',\n",
       " '12 year olds',\n",
       " '12 years hanging',\n",
       " '12 years of',\n",
       " '124 but this',\n",
       " '1286 61091 00',\n",
       " '12oz cup if',\n",
       " '12processo_eng php and',\n",
       " '13 11 bolero_pr',\n",
       " '13 days per',\n",
       " '13 extra days',\n",
       " '13 google won',\n",
       " '13 my health',\n",
       " '13 years in',\n",
       " '130 credit hours',\n",
       " '130 mod up',\n",
       " '130kw to put',\n",
       " '130kw when floor',\n",
       " '1318 emacs http',\n",
       " '14 000 sq',\n",
       " '14 10 000',\n",
       " '14 207 104',\n",
       " '14 32 49',\n",
       " '14 amp size',\n",
       " '14 do you',\n",
       " '14 pkr pakistan',\n",
       " '14 with very',\n",
       " '14107 after reading',\n",
       " '148 to 177',\n",
       " '14e2xpi usd for',\n",
       " '14th the grand',\n",
       " '15 1971 while',\n",
       " '15 amp version',\n",
       " '15 and 15',\n",
       " '15 are much',\n",
       " '15 comments that',\n",
       " '15 credit hours',\n",
       " '15 hours day',\n",
       " '15 isn prediction',\n",
       " '15 minute session',\n",
       " '15 minutes after',\n",
       " '15 minutes and',\n",
       " '15 minutes bullshit',\n",
       " '15 minutes can',\n",
       " '15 minutes gather',\n",
       " '15 years ago',\n",
       " '15 years time',\n",
       " '150 000 excepting',\n",
       " '150 bargain those',\n",
       " '150 gt acc',\n",
       " '150 mo interserver',\n",
       " '150 second your',\n",
       " '150 years have',\n",
       " '150k range for',\n",
       " '150k salaries some',\n",
       " '1512 pdf sexy',\n",
       " '15541 11 points',\n",
       " '1581345615 he probably',\n",
       " '15k month to',\n",
       " '15th click through',\n",
       " '15th of second',\n",
       " '16 72 hours',\n",
       " '16 al gore',\n",
       " '16 and had',\n",
       " '16 credits total',\n",
       " '16 grade points',\n",
       " '16 hour days',\n",
       " '16 hours as',\n",
       " '16 minutes but',\n",
       " '16 on the',\n",
       " '16 or 12',\n",
       " '16 underage drinker',\n",
       " '1600 when the',\n",
       " '1605 ggccebro chapter1',\n",
       " '1628 on the',\n",
       " '163 and try',\n",
       " '165m of philanthropy',\n",
       " '16s within minutes',\n",
       " '17 years old',\n",
       " '177 cc 25',\n",
       " '18 before my',\n",
       " '18 year old',\n",
       " '1880 with planning',\n",
       " '189cm and and',\n",
       " '19 times out',\n",
       " '191 56 41',\n",
       " '191 57 72',\n",
       " '191 58 72',\n",
       " '191 grade points',\n",
       " '191 points previously',\n",
       " '1918944 00 html',\n",
       " '192 pp gt',\n",
       " '1921 and http',\n",
       " '1921 to 1929',\n",
       " '1929 amp amount',\n",
       " '1929 inflation was',\n",
       " '1936 has been',\n",
       " '1942 is mind',\n",
       " '19459 we see',\n",
       " '1946 every line',\n",
       " '1948 it is',\n",
       " '1948 was clear',\n",
       " '1949 my recent',\n",
       " '1966 ford mustang',\n",
       " '1970 1971 foreigners',\n",
       " '1971 foreigners demanded',\n",
       " '1971 while the',\n",
       " '1974 boundary changes',\n",
       " '1978 in 2005',\n",
       " '1980s what this',\n",
       " '1981 192 pp',\n",
       " '1981 has no',\n",
       " '1981 they were',\n",
       " '1982 the posted',\n",
       " '1984 reports empire',\n",
       " '1986 and they',\n",
       " '1987 long before',\n",
       " '1989 http en',\n",
       " '1990 miata lacks',\n",
       " '1990 or before',\n",
       " '1991 dr robin',\n",
       " '1993 john wiley',\n",
       " '1993 what about',\n",
       " '1994 home of',\n",
       " '1994 or so',\n",
       " '1995 its large',\n",
       " '1996 he had',\n",
       " '1997 http usa',\n",
       " '1997 to suggest',\n",
       " '1998 it was',\n",
       " '1999 2000 at',\n",
       " '1999 definitely learned',\n",
       " '1999 he just',\n",
       " '19th century he',\n",
       " '19th century people',\n",
       " '1st amendment again',\n",
       " '1st century palestine',\n",
       " '1y amp size',\n",
       " '20 20 episode',\n",
       " '20 25 based',\n",
       " '20 and 40',\n",
       " '20 can deduce',\n",
       " '20 days 37',\n",
       " '20 episode think',\n",
       " '20 floors previously',\n",
       " '20 in the',\n",
       " '20 million in',\n",
       " '20 minutes worth',\n",
       " '20 of it',\n",
       " '20 times already',\n",
       " '20 times don',\n",
       " '20 to 60',\n",
       " '20 us fluid',\n",
       " '20 year old',\n",
       " '20 years from',\n",
       " '20 years olds',\n",
       " '200 factorial acc',\n",
       " '200 square feet',\n",
       " '200 times greater',\n",
       " '200 tip gentlemen',\n",
       " '200 with earliest',\n",
       " '200 years ago',\n",
       " '200 years don',\n",
       " '200 g of',\n",
       " '2000 at which',\n",
       " '2000 xp 2003',\n",
       " '2000 years religion',\n",
       " '2001 cholera epidemic',\n",
       " '2001 http www',\n",
       " '2001 it now',\n",
       " '2001 right reading',\n",
       " '2001 still crazy',\n",
       " '2001 version there',\n",
       " '2001 we have',\n",
       " '2002 2003 and',\n",
       " '2002 50 cdn',\n",
       " '2002 can you',\n",
       " '2002 cdn 50',\n",
       " '2002 that have',\n",
       " '2002 there was',\n",
       " '2003 and 2004',\n",
       " '2003 nt it',\n",
       " '2004 09 09_401',\n",
       " '2004 09 30',\n",
       " '2004 according to',\n",
       " '2004 and was',\n",
       " '2004 for improper',\n",
       " '2004 see http',\n",
       " '2004 version http',\n",
       " '20041015134125 http www',\n",
       " '2005 03 06',\n",
       " '2005 04 13',\n",
       " '2005 07 dangers',\n",
       " '2005 09 anthropomorphized',\n",
       " '2005 10 21',\n",
       " '2005 10 barriers_are_yo',\n",
       " '2005 11 bill',\n",
       " '2005 11 the_zen_estheti',\n",
       " '2005 12 22',\n",
       " '2005 12 23',\n",
       " '2005 12 market_utopians',\n",
       " '2005 12 new_years_perso',\n",
       " '2005 after which',\n",
       " '2005 and you',\n",
       " '2005 date is',\n",
       " '2005 he wrong',\n",
       " '2005 i686 athlon',\n",
       " '2005 melosh win',\n",
       " '2005 see http',\n",
       " '2005 that was',\n",
       " '2005 the photos',\n",
       " '2005 the stuff',\n",
       " '2005 which didn',\n",
       " '2005 x86_64 x86_64',\n",
       " '2005_03_14 article1 html',\n",
       " '2006 01 09',\n",
       " '2006 01 11',\n",
       " '2006 01 14',\n",
       " '2006 01 16',\n",
       " '2006 01 are',\n",
       " '2006 01 colbert',\n",
       " '2006 01 hindsights',\n",
       " '2006 01 think_in_weeks',\n",
       " '2006 01 update_on_tech',\n",
       " '2006 but it',\n",
       " '2006 in recognition',\n",
       " '2006 not 2001',\n",
       " '2006 or what',\n",
       " '2006 probable date',\n",
       " '2006 they are',\n",
       " '20060112 ap_on_go_pr_wh bush_29',\n",
       " '2008 we ll',\n",
       " '200k files out',\n",
       " '2010 soccer world',\n",
       " '2020 allowed in',\n",
       " '20200 and is',\n",
       " '2035 meaning if',\n",
       " '2050 how you',\n",
       " '207 104 search',\n",
       " '2074 posted months',\n",
       " '20economy israel pdf',\n",
       " '20reddit com amp',\n",
       " '20th century but',\n",
       " '20th century contemporaries',\n",
       " '20th century except',\n",
       " '20th century the',\n",
       " '20the 20economy israel',\n",
       " '21 000 homes',\n",
       " '21 10 default',\n",
       " '21 32 36',\n",
       " '21 html months',\n",
       " '21 years the',\n",
       " '22 amp go',\n",
       " '22 and still',\n",
       " '22 elsmp smp',\n",
       " '22 finalists vote',\n",
       " '22 gt the',\n",
       " '22 thing week',\n",
       " '22no happy endings',\n",
       " '22socialism 22 gt',\n",
       " '23 pound hardcover',\n",
       " '23 wikipedia_gift_spurned understand',\n",
       " '2357 took while',\n",
       " '237 cc tall',\n",
       " '24 hour shifts',\n",
       " '24 may 04',\n",
       " '24 may 06',\n",
       " '2411 this is',\n",
       " '24560 you might',\n",
       " '248 72 444',\n",
       " '249 72 458',\n",
       " '24carat co uk',\n",
       " '24dot1 com weblog',\n",
       " '25 based on',\n",
       " '25 cc 85',\n",
       " '25 days 40',\n",
       " '25 days if',\n",
       " '25 days of',\n",
       " '25 jan and',\n",
       " '25 miles an',\n",
       " '25 web sites',\n",
       " '25 year old',\n",
       " '25 years from',\n",
       " '25 years of',\n",
       " '25 years seems',\n",
       " '25 years the',\n",
       " '250 000 year',\n",
       " '250 years after',\n",
       " '250th sec with',\n",
       " '25937 together it',\n",
       " '25961 43 points',\n",
       " '25cc of milk',\n",
       " '26 years and',\n",
       " '260 houses this',\n",
       " '260 zeros moreover',\n",
       " '26599 is the',\n",
       " '26615 love the',\n",
       " '26959 points 11',\n",
       " '27 before gave',\n",
       " '27 pretty amused',\n",
       " '27147 edited to',\n",
       " '2760 while trying',\n",
       " '29 14 32',\n",
       " '29 to use',\n",
       " '2b 2b 2cc',\n",
       " '2b 2b 2cperl',\n",
       " '2b 2cc 2chtml',\n",
       " '2c and probably',\n",
       " '2c machete 2c',\n",
       " '2c mushroom mushroom',\n",
       " '2c nuclear anybody',\n",
       " '2c snake have',\n",
       " '2cagent smith 2cunderwater',\n",
       " '2castronaut 2csperm donor',\n",
       " '2cc 2b 2b',\n",
       " '2cerlang 2c and',\n",
       " '2cerlang and the',\n",
       " '2cfortran 2csmalltalk 2cruby',\n",
       " '2chaskell 2cpython 2cc',\n",
       " '2chaskell 2cpython 2cjava',\n",
       " '2chaskell 2cpython 2cperl',\n",
       " '2chaskell 2cpython 2cphp',\n",
       " '2chaskell 2cpython 2csex',\n",
       " '2chaskell 2cpython 2curine',\n",
       " '2chaskell developer 2cpython',\n",
       " '2chaskell programmer 2cpython',\n",
       " '2cjava 2cc 2b',\n",
       " '2cjava 2cphp just',\n",
       " '2clisp 2cfortran 2csmalltalk',\n",
       " '2clisp 2csmalltalk 2cruby',\n",
       " '2cphp 2cjava 2cc',\n",
       " '2cphp is quite',\n",
       " '2cphp just to',\n",
       " '2cpirate 2cagent smith',\n",
       " '2cpirate in fact',\n",
       " '2cpython 2cc 2b',\n",
       " '2cpython 2cerlang and',\n",
       " '2cpython 2cjava 2cphp',\n",
       " '2cpython 2cphp 2cjava',\n",
       " '2cpython 2cphp is',\n",
       " '2cpython 2csex then',\n",
       " '2cpython programmer to',\n",
       " '2crocket scientist 2castronaut',\n",
       " '2cruby 2cerlang 2c',\n",
       " '2cruby 2chaskell 2cpython',\n",
       " '2cruby 2cpython 2cerlang',\n",
       " '2cruby developer 2chaskell',\n",
       " '2cruby for example',\n",
       " '2cruby programmer 2chaskell',\n",
       " '2csex then laugh',\n",
       " '2csmalltalk 2cruby 2cerlang',\n",
       " '2csmalltalk 2cruby 2chaskell',\n",
       " '2csmalltalk 2cruby 2cpython',\n",
       " '2csmalltalk developer 2cruby',\n",
       " '2csmalltalk programmer 2cruby',\n",
       " '2csperm donor 2cerlang',\n",
       " '2d graphics space',\n",
       " '2kw and over',\n",
       " '2liter cylinder non',\n",
       " '2mm accuracy in',\n",
       " '2nd hand diamonds',\n",
       " '2nd paragraph of',\n",
       " '2nd rate status',\n",
       " '2y amp size',\n",
       " '30 000 000',\n",
       " '30 after they',\n",
       " '30 days of',\n",
       " '30 degrees centigrade',\n",
       " '30 fewer lines',\n",
       " '30 fun google',\n",
       " '30 have the',\n",
       " '30 hour work',\n",
       " '30 minutes to',\n",
       " '30 of kids',\n",
       " '30 second blocks',\n",
       " '30 times too',\n",
       " '30 to 70',\n",
       " '30 years ago',\n",
       " '30 years copying',\n",
       " '30 years of',\n",
       " '300 amp videoid',\n",
       " '3000 month is',\n",
       " '3000 per month',\n",
       " '30414093201 512000000000000 factorial',\n",
       " '32 36 bst',\n",
       " '32 49 utc',\n",
       " '32 node redhat',\n",
       " '32 which says',\n",
       " '33 000 square',\n",
       " '33 amp chapter',\n",
       " '33 eclipse is',\n",
       " '331 best places',\n",
       " '3339 did the',\n",
       " '34 knew it',\n",
       " '342 85 billion',\n",
       " '35 bbl lots',\n",
       " '35 points still',\n",
       " '35 years since',\n",
       " '350 million very',\n",
       " '355 cc venti',\n",
       " '36 000 sq',\n",
       " '36 bst 2005',\n",
       " '36 different kinds',\n",
       " '36 people the',\n",
       " '360 times more',\n",
       " '36k and that',\n",
       " '37 signals either',\n",
       " '37 week usually',\n",
       " '37103 reddit doesn',\n",
       " '37signals message of',\n",
       " '38 but wearing',\n",
       " '3db lower or',\n",
       " '3dblended amp field',\n",
       " '3g networks really',\n",
       " '3rd and 4th',\n",
       " '3rd century myths',\n",
       " '3rd century using',\n",
       " '3rd corner do',\n",
       " '3rd party to',\n",
       " '3rd time this',\n",
       " '3rd world as',\n",
       " '3rd world countries',\n",
       " '3x4 12 points',\n",
       " '40 000 it',\n",
       " '40 but the',\n",
       " '40 gates they',\n",
       " '40 in the',\n",
       " '40 is not',\n",
       " '40 liter tank',\n",
       " '40 of our',\n",
       " '40 of them',\n",
       " '40 productivity gain',\n",
       " '40 salary increase',\n",
       " '40 the passing',\n",
       " '40 week norway',\n",
       " '40 years and',\n",
       " '400 000 sgli',\n",
       " '400 and his',\n",
       " '400 people in',\n",
       " '400 per year',\n",
       " '400 years before',\n",
       " '4000th love that',\n",
       " '4015 for more',\n",
       " '403 is still',\n",
       " '404 errors for',\n",
       " '404 image changes',\n",
       " '41 and did',\n",
       " '41 this indicates',\n",
       " '41 to 50',\n",
       " '41505 41505 html',\n",
       " '42 inch does',\n",
       " '42 yr old',\n",
       " '4202741 stm according',\n",
       " '43 points days',\n",
       " '44 bc and',\n",
       " '44 joe perceives',\n",
       " '44 usd http',\n",
       " '44 usd in',\n",
       " '444 44 joe',\n",
       " '44cc then guess',\n",
       " '45 degree angles',\n",
       " '45 liberal on',\n",
       " '453 second time',\n",
       " '455 grade points',\n",
       " '455 or of',\n",
       " '458 46 cumulative',\n",
       " '46 cumulative gpa',\n",
       " '47 of them',\n",
       " '48 hours but',\n",
       " '49 utc 2005',\n",
       " '4d257aac09496b6d 67248daf452325d0 lnk',\n",
       " '4q cc chuck',\n",
       " '4th centuries understanding',\n",
       " '4th floor and',\n",
       " '4ths vote in',\n",
       " '4x laying back',\n",
       " '50 000 000',\n",
       " '50 45 liberal',\n",
       " '50 50 chance',\n",
       " '50 50 java',\n",
       " '50 amazon com',\n",
       " '50 bbl for',\n",
       " '50 cdn 00',\n",
       " '50 chance idiots',\n",
       " '50 chance if',\n",
       " '50 chance well',\n",
       " '50 each grade',\n",
       " '50 environmental engineering',\n",
       " '50 greatest gadgets',\n",
       " '50 group thats',\n",
       " '50 gt acc',\n",
       " '50 in the',\n",
       " '50 java and',\n",
       " '50 likely just',\n",
       " '50 loud people',\n",
       " '50 ms to',\n",
       " '50 of her',\n",
       " '50 of my',\n",
       " '50 of that',\n",
       " '50 people in',\n",
       " '50 people so',\n",
       " '50 simple riddles',\n",
       " '50 this is',\n",
       " '50 trivial math',\n",
       " '50 us today',\n",
       " '50 who care',\n",
       " '50 year old',\n",
       " '50 years than',\n",
       " '50 you can',\n",
       " '500 billion small',\n",
       " '500 comments via',\n",
       " '500 error message',\n",
       " '500 guns per',\n",
       " '500 million short',\n",
       " '500 to 000',\n",
       " '500 years ago',\n",
       " '507846 amp books',\n",
       " '51 52 if',\n",
       " '512000000000000 factorial acc',\n",
       " '52 have to',\n",
       " '52 if particular',\n",
       " '52 recursions for',\n",
       " '53 000 acres',\n",
       " '53 users splitting',\n",
       " '5385434 the economy',\n",
       " '54238 london w14',\n",
       " '5529811 0612100 507846',\n",
       " '56 16 72',\n",
       " '56 41 this',\n",
       " '56 credit hours',\n",
       " '56 speak of',\n",
       " '567 8688 you',\n",
       " '57 72 248',\n",
       " '57 grade points',\n",
       " '57 varieties of',\n",
       " '57 years kept',\n",
       " '5713383956 0000000000000 factorial',\n",
       " '58 000 000',\n",
       " '58 72 249',\n",
       " '58 grade points',\n",
       " '58 of emissions',\n",
       " '59 would consider',\n",
       " '591 cc championship',\n",
       " '598einst htm searched',\n",
       " '5b 5d dollar',\n",
       " '5b 5d gdpcp',\n",
       " '5b 5d gdpdeflation',\n",
       " '5b 5d nominalgdp',\n",
       " '5b 5d unskilled',\n",
       " '5d dollar amp',\n",
       " '5d gdpcp amp',\n",
       " '5d gdpdeflation amp',\n",
       " '5d nominalgdp amp',\n",
       " '5d unskilled amp',\n",
       " '5fc3l4jqjvuj fie engrng',\n",
       " '5fencoding utf8 amp',\n",
       " '5kw when floor',\n",
       " '5y foster friess',\n",
       " '60 000 or',\n",
       " '60 000 year',\n",
       " '60 50 45',\n",
       " '60 or so',\n",
       " '60 the issue',\n",
       " '60 times more',\n",
       " '60 watt hour',\n",
       " '600 out of',\n",
       " '6000 liters of',\n",
       " '6000 per founder',\n",
       " '60k 150k salaries',\n",
       " '60k to 150k',\n",
       " '60k year job',\n",
       " '61091 00 html',\n",
       " '63 points kinda',\n",
       " '637 14 pkr',\n",
       " '64 ad nero',\n",
       " '6475319 url index',\n",
       " '66 102 104',\n",
       " '66 books and',\n",
       " '661 of the',\n",
       " '67248daf452325d0 lnk st',\n",
       " '68k assembler caml',\n",
       " '68k machine as',\n",
       " '6v6gt com amp',\n",
       " '6v6gt com humor',\n",
       " '70 degrees centigrade',\n",
       " '70 early 80',\n",
       " '70 efficient that',\n",
       " '70 lbs overweight',\n",
       " '70 of the',\n",
       " '70 of them',\n",
       " '70 of total',\n",
       " '70 people actually',\n",
       " '7000 known for',\n",
       " '70s and 80s',\n",
       " '70s and studied',\n",
       " '72 14 207',\n",
       " '72 248 72',\n",
       " '72 249 72',\n",
       " '72 444 44',\n",
       " '72 458 46',\n",
       " '72 hours of',\n",
       " '737 when it',\n",
       " '747 landing http',\n",
       " '747 thundering about',\n",
       " '75 but this',\n",
       " '75 in each',\n",
       " '75 needed to',\n",
       " '75 of her',\n",
       " '75 posts from',\n",
       " '757 at high',\n",
       " '76 amazon not',\n",
       " '76ers recent west',\n",
       " '7750701 0092647 5fencoding',\n",
       " '78 so the',\n",
       " '7th day they',\n",
       " '80 86 http',\n",
       " '80 icl came',\n",
       " '80 in the',\n",
       " '80 in tuition',\n",
       " '80 marks and',\n",
       " '80 million the',\n",
       " '80 of the',\n",
       " '8000 words but',\n",
       " '8088 ll know',\n",
       " '8090 video flickr',\n",
       " '80mb for download',\n",
       " '80s since this',\n",
       " '81 now here',\n",
       " '8123 1918944 00',\n",
       " '85 billion yen',\n",
       " '85 us fluid',\n",
       " '85 world population',\n",
       " '86 http finance',\n",
       " '8688 you ll',\n",
       " '87 is prime',\n",
       " '87 of banning',\n",
       " '88 000 000',\n",
       " '88 an honest',\n",
       " '8818454 slashdot posts',\n",
       " '888 567 8688',\n",
       " '89 95 month',\n",
       " '89q3 heardcomet 630',\n",
       " '8hrs early so',\n",
       " '8note57 html it',\n",
       " '8oz appears on',\n",
       " '8oz size is',\n",
       " '8th floor five',\n",
       " '90 degree turn',\n",
       " '90 everyone complained',\n",
       " '90 from reading',\n",
       " '90 of it',\n",
       " '90 of legally',\n",
       " '90 of the',\n",
       " '90 of them',\n",
       " '90 of your',\n",
       " '90 per person',\n",
       " '900 years after',\n",
       " '900584 let talk',\n",
       " '90s that was',\n",
       " '911research wtc7 net',\n",
       " '93 neither of',\n",
       " '933262154439 00000000000 factorial',\n",
       " '93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000 etc for',\n",
       " '940 and going',\n",
       " '948 of course',\n",
       " '95 death rate',\n",
       " '95 month to',\n",
       " '95 of nikon',\n",
       " '95 of the',\n",
       " '96 of emissions',\n",
       " '96 to 00',\n",
       " '961001 kevorkian html',\n",
       " '97 dead from',\n",
       " '97 google loves',\n",
       " '97 the_earth_seen_from_apollo_17 jpg',\n",
       " '97mojo_400 profile14 html',\n",
       " '98 has several',\n",
       " '99 huh guess',\n",
       " '99 of all',\n",
       " '99 of dna',\n",
       " '99 of the',\n",
       " '996 btus mean',\n",
       " '9kwh liter so',\n",
       " '9za gb 00442071938940',\n",
       " '9za gb registrar',\n",
       " '______ using the',\n",
       " '_a connoisseur guide',\n",
       " '_a lot_ more',\n",
       " '_absolutely_ did not',\n",
       " '_again you feel',\n",
       " '_all this points',\n",
       " '_atheist_ there were',\n",
       " '_belief_ is needed',\n",
       " '_believe_ after all',\n",
       " '_believe_ it is',\n",
       " '_best_ clarification possible',\n",
       " '_billion_ dollars in',\n",
       " '_bit_ differently at',\n",
       " '_capitol hill blue_',\n",
       " '_chatterjee added that',\n",
       " '_christians_ _believe_ it',\n",
       " '_could_ be done',\n",
       " '_decrease_ the strength',\n",
       " '_dis_proven is no',\n",
       " '_discussed_ early submissions',\n",
       " '_explain_ the conclusions',\n",
       " '_for_ hugh ross',\n",
       " '_fox conceded that',\n",
       " '_global_ production can',\n",
       " '_highly_ contigent on',\n",
       " '_how does all',\n",
       " '_how low can',\n",
       " '_i promise you',\n",
       " '_in the future_',\n",
       " '_incredibly_ hard will',\n",
       " '_independent_ studies have',\n",
       " '_iraq_ it is',\n",
       " '_is_ instantaneous for',\n",
       " '_is_ leap into',\n",
       " '_know_ about it',\n",
       " '_list peer reviewed',\n",
       " '_long long_ time',\n",
       " '_many of teresa',\n",
       " '_mathematica_ ve got',\n",
       " '_more_ sense to',\n",
       " '_most of them',\n",
       " '_mother teresa business',\n",
       " '_next_ life is',\n",
       " '_nobody except robbie',\n",
       " '_not_ net for',\n",
       " '_not_ to use',\n",
       " '_now_ while we',\n",
       " '_okay so theresa',\n",
       " '_one question then',\n",
       " '_other people_ isn',\n",
       " '_overwhelmingly_ rich but',\n",
       " '_people who take',\n",
       " '_persistent_ early submissions',\n",
       " '_really just cut',\n",
       " '_really smart_ people',\n",
       " '_really_ believe this',\n",
       " '_really_ don want',\n",
       " '_religious_ one and',\n",
       " '_same_ set of',\n",
       " '_scientifically_ we have',\n",
       " '_should_ work and',\n",
       " '_so my hearsay',\n",
       " '_the great transformation_',\n",
       " '_the next life',\n",
       " '_their_ lives the',\n",
       " '_this_ is rhetorical',\n",
       " '_totally_ justifies the',\n",
       " '_verify_ that rectangle',\n",
       " '_why_ the mainstream',\n",
       " '_wille zur macht_',\n",
       " '_with growing fame',\n",
       " '_you are wanker_',\n",
       " '_you_ state that',\n",
       " 'a3 mobile sales',\n",
       " 'a3 msi for',\n",
       " 'a3 solution supports',\n",
       " 'a4 wastage of',\n",
       " 'a_bayes and some',\n",
       " 'aaaaaaaaaaaaaarrrrrgh most of',\n",
       " 'aac could dilute',\n",
       " 'aacs advanced access',\n",
       " 'aaron if peta',\n",
       " 'aaron what did',\n",
       " 'aaronkoblin com work',\n",
       " 'aaronsw com weblog',\n",
       " 'aaronsw would be',\n",
       " 'aasted org adblock',\n",
       " 'abandon myspace and',\n",
       " 'abandon the corporate',\n",
       " ...]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams.fit(documents)\n",
    "trigrams.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we want to transform a new test document, we can use the transform method that we previously used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = bow.transform(['this is a test document','look at me I am a test document'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can also use the tf-idf vectorizer: More on all vectorizer\n",
    "\n",
    "* The tf-idf vectorizer takes has the ability to detect words that might be more important for our specific corpus http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "\n",
    "* Places a higher weight on words that appear in certain documents that are infrequent in the overall corpus\n",
    "<img src=\"./resources/tfidf.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model with NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "#### Spacy \n",
    "\n",
    "Spacy has features related to syntactic meaning of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = 'the children were running towards the chickens when they fell down the hill'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = nlp(sample_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the DET\n",
      "children NOUN\n",
      "were VERB\n",
      "running VERB\n",
      "towards ADP\n",
      "the DET\n",
      "chickens NOUN\n",
      "when ADV\n",
      "they PRON\n",
      "fell VERB\n",
      "down ADP\n",
      "the DET\n",
      "hill NOUN\n"
     ]
    }
   ],
   "source": [
    "for word in tokenized:\n",
    "    print(word, word.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also detect things such as \"noun chunks\" and many other parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the children\n",
      "the chickens\n",
      "they\n",
      "the hill\n"
     ]
    }
   ],
   "source": [
    "for chunk in bleh.noun_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity of two documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell how similar two documents are to one another, normalizing for size, by taking the cosine similarity of the two. \n",
    "\n",
    "This number will range from [0,1], with 0 being not similar whatsoever, and 1 being the exact same.\n",
    "\n",
    "<img src=\"./resources/better_cos_similarity.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = CountVectorizer()\n",
    "sunday_afternoon = ['I ate a burger at burger queen and it was very good.',\n",
    "           'I ate a hot dog at burger prince and it was bad',\n",
    "          'I drove a racecar through your kitchen door',\n",
    "          'I ate a hot dog at burger king and it was bad. I ate a burger at burger queen and it was very good']\n",
    "\n",
    "trial.fit(sunday_afternoon)\n",
    "text_data = trial.transform(sunday_afternoon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "## the 0th and 2nd index lines are very different, a number close to 0\n",
    "cosine_similarity(text_data[0],text_data[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91413793]])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the 0th and 3rd index lines are very similar, despite different lengths\n",
    "cosine_similarity(text_data[0],text_data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
